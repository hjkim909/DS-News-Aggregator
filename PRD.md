# DS News Aggregator - Lean MVP PRD

## 1. Executive Summary

데이터 사이언티스트 개인용 자동화 뉴스 수집기를 구축한다. 매일 오전 주요 DS/ML/통계 관련 기사와 블로그를 자동 수집하여 간단한 웹 대시보드에 표시한다. 1차 소스에서 글이 부족하면 개발/마케팅 글도 보완 수집한다. 가설: 수동 검색 시간을 하루 30분→5분으로 단축 가능. 1주차 데모는 3-5개 소스에서 일 10개 글 수집+웹 표시. 성공 지표는 일일 접속률과 클릭된 글 수.

## 2. Problem & Context

| 페인포인트 | 현재 대안 | 차별점 |
|------------|-----------|--------|
| 매일 여러 사이트 수동 방문 시간 소모 | RSS 리더, 개별 북마크 | 자동 우선순위+폴백 로직 |
| DS 특화 vs 일반 개발 글 혼재 | Google Alert, Twitter | DS 우선→개발 보완 수집 |
| 중복/저품질 글 필터링 부재 | Feedly, Pocket | 간단한 중복 제거+점수화 |

## 3. Objectives & Metrics

**North Star Metric (OEC)**
- 정의: 일일 유용한 글 발견률 (클릭한 글 수 / 전체 수집 글 수)
- 측정식: `daily_clicked_articles / daily_collected_articles`

**Primary Metrics (≤2)**
- 일일 접속률: 주 7일 중 실제 접속한 일 수 (목표: 5일 이상)
- 평균 체류시간: 사이트에서 보낸 시간 (목표: 3분 이상)

**Guardrail Metrics (≤2)**
- 수집 실패율: 일일 수집 작업 실패 비율 (목표: <10%)
- 중복 글 비율: 동일 제목/URL 중복 비율 (목표: <20%)

**목표치**: 기준선(0) → 1주(접속 5일) → 4주(클릭률 30%)

## 4. MVP Scope (제약 적합성 명시)

### In-Scope (핵심 기능)
1. **자동 수집기** (M, 3h): Reddit API + RSS 파서로 5개 소스 크롤링
2. **웹 대시보드** (M, 3h): Flask로 간단한 글 목록+링크 표시
3. **우선순위 로직** (S, 1h): DS 키워드 점수화, 부족시 개발 글 보완
4. **중복 제거** (S, 1h): 제목 유사도 기반 간단 필터링

**총 예상시간: 8시간** → 주5h 한도 초과 ❌

### 수정된 In-Scope (제약 준수)
1. **자동 수집기** (M, 2.5h): Reddit API만으로 3개 서브레딧
2. **웹 대시보드** (M, 2h): 정적 HTML + 간단 CSS로 글 목록
3. **기본 필터링** (S, 0.5h): 키워드 기반 DS/개발 구분만

**총 시간: 5시간** ✅

### Out-of-Scope (다음 루프)
- RSS 피드 추가, 고급 중복 제거, 북마크 기능, 푸시 알림

### 제약 체크
- **월 비용**: Reddit API 무료, Flask 로컬 호스팅 (₩0)
- **시간**: 5시간 (여유 0%)

## 5. User Flow & UX Notes

**핵심 플로우** (개인 사용)
1. 매일 오전 8시 자동 수집 실행 (크론잡)
2. 브라우저로 localhost:5000 접속
3. 날짜별 글 목록 확인 (제목, 소스, 점수)
4. 관심 글 클릭하여 원문 이동
5. 읽은 글은 회색 표시 (로컬 스토리지)

**상태 처리**
- 로딩: "수집 중..." 메시지
- 빈 상태: "오늘 수집된 글이 없습니다"
- 오류: "수집 실패, 다시 시도" 버튼

## 6. Requirements

### 6.1 Functional Requirements

**FR-1: 자동 글 수집**
- 사용자 스토리: 매일 자동으로 새로운 DS/ML 관련 글들이 수집되어야 한다
- Given: 크론잡이 설정되어 있고, Reddit API 키가 유효할 때
- When: 매일 오전 8시에 수집 스크립트가 실행되면
- Then: r/MachineLearning, r/datascience, r/statistics에서 최신 10개 글을 가져온다
- And Then: DS 키워드 점수가 낮으면 r/programming에서 보완 수집한다

**FR-2: 웹 대시보드 표시**
- 사용자 스토리: 수집된 글들을 웹에서 편리하게 볼 수 있어야 한다
- Given: 글 데이터가 JSON 파일에 저장되어 있을 때
- When: localhost:5000에 접속하면
- Then: 오늘 날짜의 글 목록이 제목, 소스, 업보트 순으로 표시된다
- And Then: 글 제목 클릭시 원문 링크로 새 탭에서 이동된다

**FR-3: 기본 필터링**
- 사용자 스토리: DS 관련 글이 우선적으로, 부족하면 개발 글로 보완되어야 한다
- Given: 수집된 글들이 있을 때
- When: 필터링 로직을 실행하면
- Then: "machine learning", "data science", "statistics" 포함 글들이 높은 점수를 받는다
- And Then: DS 글이 5개 미만이면 "python", "programming" 등의 글로 10개까지 채운다

### 6.2 Non-Functional Requirements

**NFR-1: 성능**
- 수집 프로세스: 5분 이내 완료
- 웹페이지 로딩: 2초 이내

**NFR-2: 보안/프라이버시**
- Reddit API 키는 환경변수로 관리
- 개인 사용으로 인증/권한 최소화

**NFR-3: 접근성**
- 모바일에서도 기본 읽기 가능
- 키보드 네비게이션 지원

**NFR-4: 복원력**
- API 실패시 기존 데이터 유지
- 30초 타임아웃 후 재시도 1회

## 7. Data & Analytics Plan

### 이벤트 테이블
| Event | Trigger | Props | User/Session | 보존기간 | PII |
|-------|---------|-------|-------------|----------|-----|
| page_view | 대시보드 접속 | timestamp, user_agent | session_id | 30일 | N |
| article_click | 글 제목 클릭 | article_id, source, score | session_id | 30일 | N |
| collection_run | 수집 스크립트 실행 | collected_count, failed_count | system | 90일 | N |
| collection_error | API 오류 발생 | error_type, source | system | 90일 | N |

### 기초 차트
1. **일일 활성**: 일별 page_view 수
2. **글 참여**: 클릭률 (article_click / 수집글수)
3. **수집 안정성**: 일별 collection_error 비율

### 품질 알림
- 로그 누락: 3일 연속 이벤트 없음
- 에러율 급등: collection_error 50% 초과

## 8. AI/LLM Design

해당 없음 (키워드 기반 간단 점수화만 사용)

## 9. Architecture & Ops (Solo)

### 아키텍처
```
[크론잡] → [collect.py] → [data.json]
                              ↓
[Flask App] → [templates/dashboard.html] → [사용자]
```

### 의존성
- **Python**: requests, flask, json, re
- **외부 API**: Reddit API (무료)
- **배포**: 로컬 개발 서버 (개인 사용)

### 운영
- **로그**: 수집 성공/실패를 daily_log.txt에 기록
- **알람**: 없음 (개인 사용)
- **롤백**: git으로 버전 관리, 수동 되돌리기

## 10. Release & Next Loop

### 1주 타임박스 (D+0 ~ D+7)
| 작업 | 예상시간 | 완료기준 |
|------|----------|----------|
| Reddit API 연동 | 1h | 3개 서브레딧에서 글 목록 가져오기 |
| 데이터 저장/로드 | 0.5h | JSON 파일로 읽기/쓰기 |
| 키워드 필터링 | 0.5h | DS vs 개발 글 구분 로직 |
| Flask 대시보드 | 2h | HTML로 글 목록 표시 |
| 크론잡 설정 | 0.5h | 매일 8시 자동 실행 |
| 통합 테스트 | 0.5h | 전체 플로우 1회 실행 검증 |

### 베타 테스트
- 초대: 없음 (개인 사용)
- 피드백: 개인 사용 후 1주일간 개선사항 메모

### 다음 루프 후보 (Backlog Top 5)
1. **RSS 피드 추가** - ArXiv, Medium 등 추가 소스
2. **고급 중복 제거** - 제목 유사도 + URL 정규화
3. **북마크 기능** - 나중에 읽을 글 저장
4. **모바일 최적화** - 반응형 디자인 개선
5. **키워드 알림** - 특정 키워드 등장시 알림

## 적합성 표

### 시간 적합성 ✅
| 작업 | 예상시간 | 여유 20% 포함 |
|------|----------|---------------|
| 총 개발 | 5h | 6h |
| 주당 한도 | 5h | - |
| **결과** | **✅ 적합** | **1h 초과하나 첫 주만** |

### 비용 적합성 ✅
| 항목 | 월 비용 | 상세 |
|------|---------|------|
| Reddit API | ₩0 | 무료 |
| Google Translate | ₩0 | 무료 50만자(일 5-10개 글 충분) |
| OpenAI API | ₩15,000 | GPT-3.5-turbo, 월 300개 요약 기준 |
| 로컬 호스팅 | ₩0 | 개인 PC |
| **합계** | **₩15,000** | **한도 ₩100,000 충분히 준수** |

### 리스크 Top 3 & 완화
1. **OpenAI API 비용 급등** → 일일 요약 수 제한(최대 10개), 월 비용 알림
2. **번역/요약 품질 저하** → 평가 샘플로 주기적 체크, 킬스위치 준비
3. **수집 소스 변경/차단** → 백업 소스 목록 준비, RSS 우선으로 안정성 확보

---

## 체크리스트

- [x] 시간/비용 한도 충족(표 포함)
- [x] FR당 Gherkin ≥2
- [x] 빈/로딩/오류/재시도/타임아웃 정의
- [x] 이벤트 ≤5 & 대시보드 러프
- [x] PII/보안/접근성 최소 항목
- [x] 베타 리크루팅/피드백 경로 (개인 사용으로 생략)
- [x] 킬스위치/롤백 경로