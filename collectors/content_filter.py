#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Content Filter
ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì½˜í…ì¸  í•„í„°ë§ ì‹œìŠ¤í…œ
ê¸°ë³¸ 50ì  + ìš°ì„  í‚¤ì›Œë“œ +10~20ì  + ì†ŒìŠ¤ ê°€ì¤‘ì¹˜ - ì œì™¸ íŒ¨í„´ -30ì 
ìµœì¢… 70ì  ì´ìƒë§Œ ì„ ë³„í•˜ì—¬ 5-10ê°œ ë°˜í™˜
"""

import re
import logging
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional, Set, Tuple
from collections import Counter
from difflib import SequenceMatcher
from config import Config

logger = logging.getLogger(__name__)

class ContentFilter:
    """ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì½˜í…ì¸  í•„í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        ì½˜í…ì¸  í•„í„° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
    
    def calculate_score(self, title: str, content: str, source_id: str) -> float:
        """
        PRD v2.0 ì ìˆ˜í™” ì‹œìŠ¤í…œ - ì†ŒìŠ¤ë³„ ê¸°ë³¸ ì ìˆ˜ + í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
        ë‰´ìŠ¤ ë¯¸ë””ì–´: 100ì , ë¸”ë¡œê·¸: 80ì , ê¸°ì—…: 70ì  + í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
        
        Args:
            title: ê¸€ ì œëª©
            content: ê¸€ ë‚´ìš©  
            source_id: ì†ŒìŠ¤ ì‹ë³„ì
            
        Returns:
            ê³„ì‚°ëœ ì ìˆ˜
        """
        # PRD v2.0 - ì†ŒìŠ¤ë³„ ê¸°ë³¸ ì ìˆ˜
        base_score = self.config.SOURCE_BASE_SCORES.get(source_id, self.config.BASE_SCORE)
        score = base_score
        
        # ì „ì²´ í…ìŠ¤íŠ¸ (ì œëª© + ë‚´ìš©)
        full_text = (title + " " + content).lower()
        
        # 1. ìš°ì„  í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+10~20ì )
        for keyword, bonus in self.config.PRIORITY_KEYWORDS.items():
            if keyword.lower() in full_text:
                score += bonus
                logger.debug(f"ìš°ì„  í‚¤ì›Œë“œ '{keyword}' ë³´ë„ˆìŠ¤: +{bonus}ì ")
        
        # 2. ì œì™¸ íŒ¨í„´ íŒ¨ë„í‹° (-30ì )
        for pattern in self.config.EXCLUDE_PATTERNS:
            if pattern.lower() in full_text:
                score -= 30
                logger.debug(f"ì œì™¸ íŒ¨í„´ '{pattern}' íŒ¨ë„í‹°: -30ì ")
        
        logger.debug(f"'{source_id}' ìµœì¢… ì ìˆ˜: {score}ì  (ê¸°ë³¸ {base_score}ì )")
        
        return max(0, score)  # ìµœì†Œ 0ì 
    
    def _is_duplicate(self, article1: Dict[str, Any], article2: Dict[str, Any], 
                     similarity_threshold: float = 0.8) -> bool:
        """
        ë‘ ê¸€ì´ ì¤‘ë³µì¸ì§€ í™•ì¸
        
        Args:
            article1: ì²« ë²ˆì§¸ ê¸€
            article2: ë‘ ë²ˆì§¸ ê¸€
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ì¤‘ë³µ ì—¬ë¶€
        """
        # URLì´ ê°™ìœ¼ë©´ ì¤‘ë³µ
        if article1.get('url') == article2.get('url'):
            return True
        
        # ì œëª© ìœ ì‚¬ë„ ê²€ì‚¬
        title1 = article1.get('title', '').lower()
        title2 = article2.get('title', '').lower()
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        title1 = re.sub(r'[^\w\sê°€-í£]', '', title1)
        title2 = re.sub(r'[^\w\sê°€-í£]', '', title2)
        
        similarity = SequenceMatcher(None, title1, title2).ratio()
        
        return similarity >= similarity_threshold
    
    def _is_recent_article(self, article: Dict[str, Any]) -> bool:
        """
        ìµœê·¼ 1~2ë‹¬ ì´ë‚´ ë°œí–‰ëœ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
        
        Args:
            article: ê²€ì‚¬í•  ê¸€
            
        Returns:
            ìµœê·¼ ê¸°ì‚¬ ì—¬ë¶€
        """
        pub_date_str = article.get('published', '')
        if not pub_date_str:
            logger.debug(f"ë°œí–‰ì¼ ì—†ìŒìœ¼ë¡œ ì œì™¸: {article.get('title', '')[:50]}")
            return False
        
        try:
            # ISO í˜•ì‹ ë‚ ì§œ íŒŒì‹±
            if pub_date_str.endswith('+00:00'):
                pub_date = datetime.fromisoformat(pub_date_str.replace('+00:00', '+00:00'))
            elif 'T' in pub_date_str:
                pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
            else:
                pub_date = datetime.fromisoformat(pub_date_str)
            
            # timezoneì´ ì—†ìœ¼ë©´ UTCë¡œ ê°€ì •
            if pub_date.tzinfo is None:
                pub_date = pub_date.replace(tzinfo=timezone.utc)
            
            # í˜„ì¬ ì‹œê°„ê³¼ ë¹„êµ
            now = datetime.now(timezone.utc)
            article_age_days = (now - pub_date).days
            max_age = self.config.MAX_ARTICLE_AGE_DAYS  # ê¸°ë³¸ 60ì¼
            
            # ì—°ë„ ì²´í¬ (2025ë…„ ì´í›„ë§Œ)
            if pub_date.year < self.config.MIN_PUBLISH_YEAR:
                logger.debug(f"{pub_date.year}ë…„ ê¸°ì‚¬ë¡œ ì œì™¸: {article.get('title', '')[:50]}")
                return False
            
            # ìµœëŒ€ ë‚˜ì´ ì²´í¬
            if article_age_days > max_age:
                logger.debug(f"{article_age_days}ì¼ ì „ ê¸°ì‚¬ë¡œ ì œì™¸ (ìµœëŒ€: {max_age}ì¼): {article.get('title', '')[:50]}")
                return False
            
            logger.debug(f"ìµœì‹  ê¸°ì‚¬ ìŠ¹ì¸ ({article_age_days}ì¼ ì „): {article.get('title', '')[:50]}")
            return True
            
        except (ValueError, TypeError) as e:
            logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ '{pub_date_str}': {e}")
            return False
    
    def _has_ds_ml_keywords(self, article: Dict[str, Any], strict_mode: bool = True) -> bool:
        """
        DS/ML/LLM/AI ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ ì—„ê²©í•˜ê²Œ í™•ì¸
        
        Args:
            article: ê²€ì‚¬í•  ê¸€
            strict_mode: ì—„ê²© ëª¨ë“œ (ê¸°ë³¸ê°’: True)
            
        Returns:
            DS/ML/LLM/AI í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        """
        title = article.get('title', '').lower()
        content = article.get('content', '').lower()
        full_text = title + ' ' + content
        
        # ì—„ê²© ëª¨ë“œ: í•µì‹¬ AI/ML/LLM í‚¤ì›Œë“œë§Œ í—ˆìš©
        if strict_mode:
            # 1ë‹¨ê³„: ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (ì¼ë°˜ ê°œë°œ ë‚´ìš© ì œì™¸)
            excluded_keywords = getattr(self.config, 'EXCLUDED_TECH_KEYWORDS', [])
            for excluded_keyword in excluded_keywords:
                if excluded_keyword.lower() in full_text:
                    logger.debug(f"ì œì™¸ í‚¤ì›Œë“œë¡œ ê±°ë¶€: {excluded_keyword}")
                    return False
            
            # 2ë‹¨ê³„: í•µì‹¬ AI/ML/LLM í‚¤ì›Œë“œ í™•ì¸
            required_keyword_found = False
            core_keywords = [
                # í•µì‹¬ AI/ML
                'machine learning', 'deep learning', 'artificial intelligence', 'neural network',
                'data science', 'llm', 'large language model', 'gpt', 'transformer',
                'computer vision', 'natural language processing', 'nlp',
                
                # í•œêµ­ì–´ í•µì‹¬ í‚¤ì›Œë“œ  
                'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì¸ê³µì§€ëŠ¥', 'llm', 'ëŒ€í˜•ì–¸ì–´ëª¨ë¸', 'ìƒì„±í˜•ai',
                'ìì—°ì–´ì²˜ë¦¬', 'ì»´í“¨í„°ë¹„ì „', 'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤'
            ]
            
            for keyword in core_keywords:
                if keyword.lower() in full_text:
                    required_keyword_found = True
                    logger.debug(f"í•µì‹¬ í‚¤ì›Œë“œ ë°œê²¬: {keyword}")
                    break
            
            if not required_keyword_found:
                logger.debug(f"í•µì‹¬ AI/ML í‚¤ì›Œë“œ ë¶€ì¡±ìœ¼ë¡œ ê±°ë¶€")
                return False
        
        # 3ë‹¨ê³„: ì¶”ê°€ DS/ML í‚¤ì›Œë“œ í™•ì¸
        for keyword in self.config.DS_KEYWORDS:
            if keyword.lower() in full_text:
                return True
        
        return False
    
    def remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì¤‘ë³µ ê¸€ ì œê±°
        
        Args:
            articles: ê¸€ ëª©ë¡
            
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ ê¸€ ëª©ë¡
        """
        if not articles:
            return []
        
        unique_articles = []
        
        for article in articles:
            is_duplicate = False
            
            for existing_article in unique_articles:
                if self._is_duplicate(article, existing_article):
                    # ë” ë†’ì€ ì ìˆ˜ì˜ ê¸€ì„ ìœ ì§€
                    if article.get('score', 0) > existing_article.get('score', 0):
                        unique_articles.remove(existing_article)
                        unique_articles.append(article)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
        
        logger.info(f"ì¤‘ë³µ ì œê±°: {len(articles)} â†’ {len(unique_articles)}ê°œ")
        return unique_articles
    
    def filter_by_score_threshold(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì ìˆ˜ ì„ê³„ê°’(70ì  ì´ìƒ)ìœ¼ë¡œ í•„í„°ë§
        
        Args:
            articles: í•„í„°ë§í•  ê¸€ ëª©ë¡
            
        Returns:
            ì„ê³„ê°’ ì´ìƒì˜ ê¸€ ëª©ë¡
        """
        threshold = self.config.MIN_SCORE_THRESHOLD  # 70ì 
        
        filtered = [
            article for article in articles 
            if article.get('score', 0) >= threshold
        ]
        
        logger.info(f"ì ìˆ˜ í•„í„°ë§: {len(articles)} â†’ {len(filtered)}ê°œ (ì„ê³„ê°’: {threshold}ì )")
        return filtered
    
    def filter_all_articles_by_keywords(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  ê¸€ì—ì„œ AI/ML/LLM ì£¼ì œë§Œ ì—„ê²©í•˜ê²Œ í•„í„°ë§ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
        
        Args:
            articles: í•„í„°ë§í•  ê¸€ ëª©ë¡
            
        Returns:
            AI/ML/LLM ì£¼ì œë¡œ í•„í„°ë§ëœ ê¸€ ëª©ë¡
        """
        filtered = []
        rejected_count = 0
        
        for article in articles:
            # 1ë‹¨ê³„: ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ 1~2ë‹¬)
            if not self._is_recent_article(article):
                rejected_count += 1
                logger.debug(f"ê±°ë¶€: {article.get('title', '')[:50]} (ì˜¤ë˜ëœ ê¸°ì‚¬)")
                continue
            
            # 2ë‹¨ê³„: ì—„ê²©í•œ AI/ML/LLM í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
            if self._has_ds_ml_keywords(article, strict_mode=True):
                filtered.append(article)
                logger.debug(f"ìŠ¹ì¸: {article.get('title', '')[:50]} (ì†ŒìŠ¤: {article.get('source', '')})")
            else:
                rejected_count += 1
                logger.debug(f"ê±°ë¶€: {article.get('title', '')[:50]} (AI/ML/LLM ì£¼ì œ ì•„ë‹˜)")
        
        logger.info(f"ğŸ¯ ë‚ ì§œ+AI/ML/LLM í•„í„°ë§: {len(articles)} â†’ {len(filtered)}ê°œ (ê±°ë¶€: {rejected_count}ê°œ)")
        return filtered
    
    def filter_korean_blogs_by_keywords(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        í•œêµ­ ë¸”ë¡œê·¸ ê¸€ì—ì„œ DS/ML í‚¤ì›Œë“œ í¬í•¨ ê¸€ë§Œ í•„í„°ë§ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
        
        Args:
            articles: í•„í„°ë§í•  ê¸€ ëª©ë¡
            
        Returns:
            í‚¤ì›Œë“œ í•„í„°ë§ëœ ê¸€ ëª©ë¡
        """
        # ì´ì œ ëª¨ë“  ì†ŒìŠ¤ì— ì—„ê²©í•œ í•„í„°ë§ ì ìš©
        return self.filter_all_articles_by_keywords(articles)
    
    def get_top_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìµœì¢… í•„í„°ë§
        70ì  ì´ìƒ, ì¤‘ë³µ ì œê±° í›„ 5-10ê°œ ë°˜í™˜
        
        Args:
            articles: ê¸€ ëª©ë¡
            
        Returns:
            ìµœì¢… ì„ ë³„ëœ ê¸€ ëª©ë¡
        """
        if not articles:
            return []
        
        logger.info(f"===== ì½˜í…ì¸  í•„í„°ë§ ì‹œì‘: {len(articles)}ê°œ ê¸€ =====")
        
        # 1ë‹¨ê³„: ì—„ê²©í•œ AI/ML/LLM ì£¼ì œ í•„í„°ë§ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
        filtered_articles = self.filter_all_articles_by_keywords(articles)
        
        # 2ë‹¨ê³„: ì ìˆ˜ ì„ê³„ê°’ í•„í„°ë§ (70ì  ì´ìƒ)
        filtered_articles = self.filter_by_score_threshold(filtered_articles)
        
        # 3ë‹¨ê³„: ì¤‘ë³µ ì œê±°
        filtered_articles = self.remove_duplicates(filtered_articles)
        
        # 4ë‹¨ê³„: ì ìˆ˜ìˆœ ì •ë ¬
        filtered_articles.sort(key=lambda x: x.get('score', 0), reverse=True)
        
        # 5ë‹¨ê³„: ìµœì¢… ê°œìˆ˜ ì œí•œ (5-10ê°œ)
        final_count = min(len(filtered_articles), self.config.FINAL_ARTICLE_COUNT)
        filtered_articles = filtered_articles[:final_count]
        
        logger.info(f"===== í•„í„°ë§ ì™„ë£Œ: ìµœì¢… {len(filtered_articles)}ê°œ ê¸€ ì„ ë³„ =====")
        
        # ê²°ê³¼ ìš”ì•½
        if filtered_articles:
            avg_score = sum(article.get('score', 0) for article in filtered_articles) / len(filtered_articles)
            sources = Counter(article.get('source', 'unknown') for article in filtered_articles)
            
            logger.info(f"í‰ê·  ì ìˆ˜: {avg_score:.1f}")
            logger.info(f"ì†ŒìŠ¤ë³„ ë¶„í¬: {dict(sources)}")
        
        return filtered_articles
    
    def analyze_filtering_results(self, original_articles: List[Dict[str, Any]], 
                                 filtered_articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        í•„í„°ë§ ê²°ê³¼ ë¶„ì„
        
        Args:
            original_articles: ì›ë³¸ ê¸€ ëª©ë¡
            filtered_articles: í•„í„°ë§ëœ ê¸€ ëª©ë¡
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if not original_articles:
            return {'error': 'ë¶„ì„í•  ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.'}
        
        # í†µê³„ ê³„ì‚°
        original_scores = [article.get('score', 0) for article in original_articles]
        filtered_scores = [article.get('score', 0) for article in filtered_articles]
        
        original_sources = Counter(article.get('source', 'unknown') for article in original_articles)
        filtered_sources = Counter(article.get('source', 'unknown') for article in filtered_articles)
        
        analysis = {
            'original_count': len(original_articles),
            'filtered_count': len(filtered_articles),
            'filter_ratio': len(filtered_articles) / len(original_articles) * 100,
            'score_stats': {
                'original': {
                    'min': min(original_scores) if original_scores else 0,
                    'max': max(original_scores) if original_scores else 0,
                    'avg': sum(original_scores) / len(original_scores) if original_scores else 0
                },
                'filtered': {
                    'min': min(filtered_scores) if filtered_scores else 0,
                    'max': max(filtered_scores) if filtered_scores else 0,
                    'avg': sum(filtered_scores) / len(filtered_scores) if filtered_scores else 0
                }
            },
            'source_distribution': {
                'original': dict(original_sources),
                'filtered': dict(filtered_sources)
            },
            'threshold': self.config.MIN_SCORE_THRESHOLD
        }
        
        return analysis


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def filter_articles(articles: List[Dict[str, Any]], config: Config = None) -> List[Dict[str, Any]]:
    """
    ê¸€ í•„í„°ë§ í¸ì˜ í•¨ìˆ˜
    
    Args:
        articles: í•„í„°ë§í•  ê¸€ ëª©ë¡
        config: ì„¤ì • ê°ì²´
        
    Returns:
        í•„í„°ë§ëœ ê¸€ ëª©ë¡
    """
    content_filter = ContentFilter(config)
    return content_filter.get_top_articles(articles)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_articles = [
        {
            'id': 'test_1',
            'title': 'ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ë°ì´í„° ë¶„ì„ ê°€ì´ë“œ',
            'content': 'LLMê³¼ ì‹œê³„ì—´ ë¶„ì„ì— ëŒ€í•œ êµ¬í˜„ ë°©ë²•...',
            'score': 85,
            'source': 'naver_d2'
        },
        {
            'id': 'test_2', 
            'title': 'ì¶”ì²œí•´ì£¼ì„¸ìš” - ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì´ ì¢‹ì„ê¹Œìš”?',
            'content': 'ì§ˆë¬¸ìˆì–´ìš”...',
            'score': 40,
            'source': 'reddit'
        },
        {
            'id': 'test_3',
            'title': 'Pythonì„ ì´ìš©í•œ ë”¥ëŸ¬ë‹ êµ¬í˜„',
            'content': 'ì‹ ê²½ë§ ë¶„ì„ ë° ë¹„êµ...',
            'score': 75,
            'source': 'kakao_tech'
        }
    ]
    
    content_filter = ContentFilter()
    
    filtered = content_filter.get_top_articles(test_articles)
    analysis = content_filter.analyze_filtering_results(test_articles, filtered)
    
    print(f"í•„í„°ë§ ê²°ê³¼: {len(filtered)}ê°œ ê¸€")
    print(f"ë¶„ì„ ê²°ê³¼: {analysis}")
    
    for article in filtered:
        print(f"- {article['title'][:50]} (ì ìˆ˜: {article['score']}, ì†ŒìŠ¤: {article['source']})")