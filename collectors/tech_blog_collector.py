#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Tech Blog Collector
ê¸€ë¡œë²Œ ê¸°ìˆ  ë¸”ë¡œê·¸ì—ì„œ RSS í”¼ë“œë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘
"""

import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any
import requests
import feedparser
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse

from config import Config
from collectors.content_filter import ContentFilter

logger = logging.getLogger(__name__)

class TechBlogCollector:
    """ê¸°ìˆ  ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        ê¸°ìˆ  ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        self.content_filter = ContentFilter(config)
        
        # ìš”ì²­ í—¤ë” (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, text/html, application/xhtml+xml, */*',
            'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # ì„¸ì…˜ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©)
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ë‚ ì§œ í•„í„°ë§ ì„¤ì •
        self.cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.config.MAX_ARTICLE_AGE_DAYS)
        
        # DS/ML ê´€ë ¨ í‚¤ì›Œë“œ (ì˜ì–´ + í•œêµ­ì–´)
        self.ds_keywords = self.config.DS_KEYWORDS + self.config.TECH_KEYWORDS + [
            'transformer', 'bert', 'gpt', 'llm', 'nlp', 'cv', 'ai', 'ml',
            'ë”¥ëŸ¬ë‹', 'AI', 'ML', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'NLP', 'CV'
        ]
    
    def _fetch_rss_feed(self, rss_url: str, source_name: str) -> Optional[Any]:
        """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜´"""
        try:
            logger.info(f"{source_name} RSS í”¼ë“œ íŒŒì‹± ì‹œì‘: {rss_url}")
            
            # feedparserëŠ” ìì²´ì ìœ¼ë¡œ HTTP ìš”ì²­ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                logger.warning(f"{source_name} RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                logger.warning(f"{source_name}ì—ì„œ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            return feed
            
        except Exception as e:
            logger.error(f"{source_name} RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_content_from_url(self, url: str) -> str:
        """ì›¹í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë³¸ë¬¸ ì„ íƒì ì‹œë„
            content_selectors = [
                'article',
                '[role="main"]',
                '.post-content',
                '.entry-content', 
                '.article-content',
                '.content',
                'main',
                '.post-body',
                '#content'
            ]
            
            content_text = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                    text_parts = []
                    for elem in elements:
                        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                        for script in elem(["script", "style", "nav", "footer", "header"]):
                            script.decompose()
                        
                        text = elem.get_text(separator=' ', strip=True)
                        if text and len(text) > 100:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            text_parts.append(text)
                    
                    content_text = ' '.join(text_parts)
                    break
            
            # ë°±ì—…: ì „ì²´ bodyì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not content_text:
                body = soup.find('body')
                if body:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in body(["script", "style", "nav", "footer", "header", "aside"]):
                        tag.decompose()
                    content_text = body.get_text(separator=' ', strip=True)
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content_text = re.sub(r'\s+', ' ', content_text).strip()
            
            # ë„ˆë¬´ ì§§ìœ¼ë©´ ì œëª©+ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
            if len(content_text) < 200:
                return ""
            
            # ìµœëŒ€ 2000ìë¡œ ì œí•œ
            return content_text[:2000] if len(content_text) > 2000 else content_text
            
        except Exception as e:
            logger.warning(f"URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ""
    
    def _extract_tags(self, title: str, content: str, source_id: str) -> List[str]:
        """ì œëª©ê³¼ ë‚´ìš©ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = set()
        text = (title + " " + content).lower()
        
        # AI/ML ê´€ë ¨ íƒœê·¸
        ml_tags = {
            'machine learning': 'ML', 'ml': 'ML', 'ë¨¸ì‹ ëŸ¬ë‹': 'ML',
            'deep learning': 'Deep Learning', 'ë”¥ëŸ¬ë‹': 'Deep Learning',
            'neural network': 'Neural Network', 'ì‹ ê²½ë§': 'Neural Network',
            'artificial intelligence': 'AI', 'ai': 'AI', 'ì¸ê³µì§€ëŠ¥': 'AI',
            'data science': 'Data Science', 'ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤': 'Data Science',
            'natural language processing': 'NLP', 'nlp': 'NLP', 'ìì—°ì–´ì²˜ë¦¬': 'NLP',
            'computer vision': 'Computer Vision', 'cv': 'Computer Vision', 'ì»´í“¨í„°ë¹„ì „': 'Computer Vision',
            'reinforcement learning': 'Reinforcement Learning', 'ê°•í™”í•™ìŠµ': 'Reinforcement Learning',
            'time series': 'Time Series', 'ì‹œê³„ì—´': 'Time Series',
            'llm': 'LLM', 'large language model': 'LLM', 'ëŒ€í˜•ì–¸ì–´ëª¨ë¸': 'LLM',
            'transformer': 'Transformer', 'bert': 'BERT', 'gpt': 'GPT'
        }
        
        for keyword, tag in ml_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ê¸°ìˆ  ìŠ¤íƒ íƒœê·¸
        tech_tags = {
            'python': 'Python', 'tensorflow': 'TensorFlow', 'pytorch': 'PyTorch',
            'keras': 'Keras', 'scikit-learn': 'Scikit-learn', 'pandas': 'Pandas',
            'numpy': 'NumPy', 'jupyter': 'Jupyter', 'docker': 'Docker',
            'kubernetes': 'Kubernetes', 'aws': 'AWS', 'cloud': 'Cloud',
            'api': 'API', 'rest': 'REST', 'graphql': 'GraphQL'
        }
        
        for keyword, tag in tech_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ì†ŒìŠ¤ë³„ íƒœê·¸
        source_tags = {
            'google_ai': 'Google AI',
            'openai': 'OpenAI', 
            'netflix_tech': 'Netflix',
            'uber_eng': 'Uber',
            'naver_d2': 'ë„¤ì´ë²„',
            'kakao_tech': 'ì¹´ì¹´ì˜¤'
        }
        
        if source_id in source_tags:
            tags.add(source_tags[source_id])
        
        return list(tags)
    
    def _calculate_score(self, title: str, content: str, source_id: str) -> float:
        """ê¸€ ì ìˆ˜ ê³„ì‚°"""
        return self.content_filter.calculate_score(title, content, source_id)
    
    def _has_ds_keywords(self, title: str, content: str) -> bool:
        """DS/ML ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        text = (title + " " + content).lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ (ë” ìœ ì—°í•˜ê²Œ)
        for keyword in self.ds_keywords:
            if keyword.lower() in text:
                return True
        
        # íŒ¨í„´ ë§¤ì¹­
        patterns = [
            r'machine\s+learning', r'deep\s+learning', r'neural\s+network',
            r'data\s+science', r'artificial\s+intelligence',
            r'time\s+series', r'computer\s+vision',
            r'natural\s+language', r'reinforcement\s+learning'
        ]
        
        for pattern in patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def collect_from_source(self, source_config: Dict[str, str], limit: int = 10) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ê¸€ ìˆ˜ì§‘"""
        source_name = source_config['name']
        source_id = source_config['source_id']
        rss_url = source_config['rss']
        
        logger.info(f"{source_name}ì—ì„œ ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        feed = self._fetch_rss_feed(rss_url, source_name)
        if not feed:
            return []
        
        articles = []
        processed_count = 0
        
        for entry in feed.entries[:limit * 2]:  # ì—¬ë¶„ìœ¼ë¡œ ë” ë§ì´ ì²˜ë¦¬
            try:
                processed_count += 1
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.get('title', '').strip()
                if not title:
                    continue
                
                # URL ì •ë¦¬
                link = entry.get('link', '')
                if not link:
                    continue
                
                # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ
                summary = entry.get('summary', '') or entry.get('description', '')
                
                # ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
                content = ""
                if summary:
                    content = BeautifulSoup(summary, 'html.parser').get_text(strip=True)
                
                # ì¶”ê°€ ë³¸ë¬¸ì´ í•„ìš”í•œ ê²½ìš° ì›¹í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
                if len(content) < 300:
                    web_content = self._extract_content_from_url(link)
                    if web_content:
                        content = web_content
                    elif summary:
                        content = BeautifulSoup(summary, 'html.parser').get_text(strip=True)
                
                # DS/ML í‚¤ì›Œë“œ í•„í„°ë§
                if not self._has_ds_keywords(title, content):
                    continue
                
                # ë‚ ì§œ íŒŒì‹±
                published_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published_time = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                else:
                    published_time = datetime.now(timezone.utc)
                
                # ë‚ ì§œ í•„í„°ë§ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: ìµœê·¼ 1~2ë‹¬)
                if published_time.year < self.config.MIN_PUBLISH_YEAR:
                    logger.debug(f"{published_time.year}ë…„ ê¸°ì‚¬ ì œì™¸: {title[:50]}")
                    continue
                    
                article_age_days = (datetime.now(timezone.utc) - published_time).days
                if article_age_days > self.config.MAX_ARTICLE_AGE_DAYS:
                    logger.debug(f"{article_age_days}ì¼ ì „ ê¸°ì‚¬ ì œì™¸: {title[:50]}")
                    continue
                
                # íƒœê·¸ ì¶”ì¶œ
                tags = self._extract_tags(title, content, source_id)
                
                # ì ìˆ˜ ê³„ì‚°
                score = self._calculate_score(title, content, source_id)
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'id': f"{source_id}_{int(time.time())}_{processed_count}",
                    'title': title,
                    'title_ko': title,  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'content': content[:2000],  # ìµœëŒ€ 2000ì
                    'content_ko': content[:2000],  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'summary': '',  # ìš”ì•½ì€ ë‚˜ì¤‘ì—
                    'url': link,
                    'source': source_id,
                    'tags': tags,
                    'score': score,
                    'published': published_time.isoformat(),
                    'collected_at': datetime.now(timezone.utc).isoformat()
                }
                
                if score > 0:  # ìµœì†Œ ì ìˆ˜ í†µê³¼
                    articles.append(article_data)
                
                # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¢…ë£Œ
                if len(articles) >= limit:
                    break
                    
            except Exception as e:
                logger.error(f"{source_name} ê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"{source_name}ì—ì„œ {len(articles)}ê°œ ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return articles
    
    def collect_all_sources(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ê¸°ìˆ  ë¸”ë¡œê·¸ ì†ŒìŠ¤ì—ì„œ ê¸€ ìˆ˜ì§‘"""
        all_articles = []
        
        for source_config in self.config.TECH_BLOG_SOURCES:
            try:
                articles = self.collect_from_source(
                    source_config, 
                    limit=self.config.MAX_ARTICLES_PER_SOURCE // len(self.config.TECH_BLOG_SOURCES)
                )
                all_articles.extend(articles)
                
                # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"ì†ŒìŠ¤ {source_config['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ê¸°ìˆ  ë¸”ë¡œê·¸ì—ì„œ ì´ {len(all_articles)}ê°œ ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles


if __name__ == '__main__':
    # ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸
    config = Config()
    collector = TechBlogCollector(config)
    
    print("ğŸ§ª ê¸°ìˆ  ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ë‹¨ì¼ ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸
    test_source = {
        'name': 'Google AI Blog',
        'rss': 'https://ai.googleblog.com/feeds/posts/default',
        'source_id': 'google_ai'
    }
    
    articles = collector.collect_from_source(test_source, limit=3)
    print(f"\nìˆ˜ì§‘ëœ ê¸€ ìˆ˜: {len(articles)}")
    
    for i, article in enumerate(articles, 1):
        print(f"\n{i}. {article['title'][:80]}...")
        print(f"   ì ìˆ˜: {article['score']:.1f}")
        print(f"   íƒœê·¸: {', '.join(article['tags'])}")
        print(f"   URL: {article['url']}")
    
    print(f"\nâœ… ê¸°ìˆ  ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
