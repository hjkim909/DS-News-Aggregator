#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Medium Collector
Medium ê³„ì—´ í”Œë«í¼ì—ì„œ RSS í”¼ë“œë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘
"""

import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any
import requests
import feedparser
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse

from config import Config
from collectors.content_filter import ContentFilter

logger = logging.getLogger(__name__)

class MediumCollector:
    """Medium ê³„ì—´ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        Medium ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        self.content_filter = ContentFilter(config)
        
        # ìš”ì²­ í—¤ë” (Mediumì€ ë´‡ ì°¨ë‹¨ì´ ìˆì–´ì„œ ì‹¤ì œ ë¸Œë¼ìš°ì € í‰ë‚´)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, text/html, application/xhtml+xml, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ë‚ ì§œ í•„í„°ë§ ì„¤ì •
        self.cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.config.MAX_ARTICLE_AGE_DAYS)
        
        # Medium íŠ¹í™” í‚¤ì›Œë“œ (ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ML, AI ì¤‘ì‹¬)
        self.medium_keywords = [
            # AI/ML í•µì‹¬
            'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'data science', 'data analysis',
            'natural language processing', 'computer vision', 'reinforcement learning',
            
            # ìµœì‹  íŠ¸ë Œë“œ
            'llm', 'large language model', 'gpt', 'bert', 'transformer',
            'chatgpt', 'openai', 'generative ai', 'stable diffusion',
            
            # ê¸°ìˆ  ìŠ¤íƒ
            'python', 'tensorflow', 'pytorch', 'scikit-learn', 'pandas',
            'numpy', 'jupyter', 'keras', 'hugging face',
            
            # ë°ì´í„° ê´€ë ¨
            'big data', 'data engineering', 'data visualization', 'analytics',
            'time series', 'predictive modeling', 'feature engineering',
            
            # ì‘ìš© ë¶„ì•¼
            'recommendation system', 'fraud detection', 'image recognition',
            'speech recognition', 'autonomous', 'fintech', 'biotech'
        ]
    
    def _fetch_medium_rss(self, rss_url: str, source_name: str) -> Optional[Any]:
        """Medium RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜´ (íŠ¹ë³„ ì²˜ë¦¬)"""
        try:
            logger.info(f"{source_name} Medium RSS í”¼ë“œ íŒŒì‹± ì‹œì‘: {rss_url}")
            
            # Medium RSSëŠ” ë•Œë•Œë¡œ User-Agentê°€ ì¤‘ìš”í•¨
            response = self.session.get(rss_url, timeout=15)
            response.raise_for_status()
            
            # feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(response.text)
            
            if feed.bozo:
                logger.warning(f"{source_name} RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                logger.warning(f"{source_name}ì—ì„œ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            logger.info(f"{source_name}ì—ì„œ {len(feed.entries)}ê°œ ê¸€ ë°œê²¬")
            return feed
            
        except Exception as e:
            logger.error(f"{source_name} RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_medium_content(self, url: str) -> str:
        """Medium ê¸€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (Medium íŠ¹í™”)"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Medium íŠ¹í™” ë³¸ë¬¸ ì„ íƒì
            medium_selectors = [
                'article',
                '[data-testid="storyContent"]',
                '.postArticle-content',
                '.section-content',
                '.graf',  # Mediumì˜ ë¬¸ë‹¨ í´ë˜ìŠ¤
                '[data-selectable-paragraph]'
            ]
            
            content_parts = []
            
            for selector in medium_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        # Medium íŠ¹í™” ì •ë¦¬
                        for unwanted in elem(['script', 'style', 'figure', 'figcaption', 'nav', 'footer']):
                            unwanted.decompose()
                        
                        text = elem.get_text(separator=' ', strip=True)
                        if text and len(text) > 50:
                            content_parts.append(text)
                    
                    if content_parts:
                        break
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            if content_parts:
                content_text = ' '.join(content_parts)
                content_text = re.sub(r'\s+', ' ', content_text).strip()
                
                # Mediumì€ ë³´í†µ ê¸´ ê¸€ì´ë¯€ë¡œ ë” ë§ì´ í—ˆìš©
                return content_text[:3000] if len(content_text) > 3000 else content_text
            
            return ""
            
        except Exception as e:
            logger.warning(f"Medium URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ""
    
    def _extract_medium_tags(self, title: str, content: str, source_id: str) -> List[str]:
        """Medium ê¸€ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = set()
        text = (title + " " + content).lower()
        
        # AI/ML ì „ë¬¸ íƒœê·¸
        ai_ml_tags = {
            'artificial intelligence': 'AI',
            'machine learning': 'Machine Learning',
            'deep learning': 'Deep Learning',
            'neural network': 'Neural Networks',
            'data science': 'Data Science',
            'natural language processing': 'NLP',
            'computer vision': 'Computer Vision',
            'reinforcement learning': 'Reinforcement Learning',
            'time series': 'Time Series',
            'generative ai': 'Generative AI',
            'large language model': 'LLM',
            'transformer': 'Transformers'
        }
        
        for keyword, tag in ai_ml_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ìµœì‹  AI ëª¨ë¸/ë„êµ¬
        model_tags = {
            'gpt': 'GPT', 'chatgpt': 'ChatGPT', 'bert': 'BERT',
            'llama': 'LLaMA', 'stable diffusion': 'Stable Diffusion',
            'midjourney': 'Midjourney', 'openai': 'OpenAI',
            'anthropic': 'Anthropic', 'claude': 'Claude'
        }
        
        for keyword, tag in model_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ê¸°ìˆ  ìŠ¤íƒ
        tech_stack_tags = {
            'python': 'Python', 'tensorflow': 'TensorFlow', 'pytorch': 'PyTorch',
            'scikit-learn': 'Scikit-learn', 'pandas': 'Pandas', 'numpy': 'NumPy',
            'jupyter': 'Jupyter', 'keras': 'Keras', 'hugging face': 'Hugging Face',
            'streamlit': 'Streamlit', 'plotly': 'Plotly', 'matplotlib': 'Matplotlib'
        }
        
        for keyword, tag in tech_stack_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # Medium ì†ŒìŠ¤ë³„ íƒœê·¸
        source_tags = {
            'towards_ds': 'Towards Data Science',
            'better_prog': 'Better Programming',
            'the_startup': 'The Startup'
        }
        
        if source_id in source_tags:
            tags.add(source_tags[source_id])
        
        return list(tags)
    
    def _calculate_medium_score(self, title: str, content: str, source_id: str) -> float:
        """Medium ê¸€ íŠ¹í™” ì ìˆ˜ ê³„ì‚°"""
        score = self.content_filter.calculate_score(title, content, source_id)
        
        # Medium íŠ¹í™” ë³´ë„ˆìŠ¤
        text = (title + " " + content).lower()
        
        # íŠœí† ë¦¬ì–¼/ê°€ì´ë“œ ë³´ë„ˆìŠ¤
        tutorial_keywords = ['tutorial', 'guide', 'how to', 'step by step', 'beginner', 'complete guide']
        for keyword in tutorial_keywords:
            if keyword in text:
                score += 10
                break
        
        # ì‹¤ë¬´ ê²½í—˜ ë³´ë„ˆìŠ¤
        experience_keywords = ['production', 'real world', 'case study', 'lessons learned', 'best practices']
        for keyword in experience_keywords:
            if keyword in text:
                score += 15
                break
        
        # ìµœì‹  ê¸°ìˆ  ë³´ë„ˆìŠ¤
        latest_tech = ['2024', '2023', 'latest', 'new', 'recent', 'state-of-the-art', 'cutting-edge']
        for keyword in latest_tech:
            if keyword in text:
                score += 5
                break
        
        return score
    
    def _has_medium_keywords(self, title: str, content: str) -> bool:
        """Medium ì „ìš© í‚¤ì›Œë“œ í•„í„°ë§"""
        text = (title + " " + content).lower()
        
        # í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in self.medium_keywords:
            if keyword in text:
                return True
        
        # íŒ¨í„´ ê¸°ë°˜ ë§¤ì¹­
        patterns = [
            r'machine\s+learning', r'deep\s+learning', r'data\s+science',
            r'artificial\s+intelligence', r'neural\s+network', r'time\s+series',
            r'natural\s+language', r'computer\s+vision', r'large\s+language\s+model'
        ]
        
        for pattern in patterns:
            if re.search(pattern, text):
                return True
        
        return False
    
    def collect_from_medium_source(self, source_config: Dict[str, str], limit: int = 8) -> List[Dict[str, Any]]:
        """Medium ì†ŒìŠ¤ì—ì„œ ê¸€ ìˆ˜ì§‘"""
        source_name = source_config['name']
        source_id = source_config['source_id']
        rss_url = source_config['rss']
        
        logger.info(f"{source_name}ì—ì„œ ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        feed = self._fetch_medium_rss(rss_url, source_name)
        if not feed:
            return []
        
        articles = []
        processed_count = 0
        
        # Mediumì€ ë³´í†µ ê³ í’ˆì§ˆ ê¸€ì´ ë§ìœ¼ë¯€ë¡œ ë” ë§ì´ ì²˜ë¦¬
        for entry in feed.entries[:limit * 3]:
            try:
                processed_count += 1
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.get('title', '').strip()
                if not title:
                    continue
                
                # URL ì •ë¦¬
                link = entry.get('link', '')
                if not link:
                    continue
                
                # Medium URL ì •ë¦¬ (íŒŒë¼ë¯¸í„° ì œê±°)
                if '?' in link:
                    link = link.split('?')[0]
                
                # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ
                summary = entry.get('summary', '') or entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
                
                if summary:
                    content = BeautifulSoup(summary, 'html.parser').get_text(strip=True)
                else:
                    content = ""
                
                # Mediumì€ RSSì— ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë¯€ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ì€ ì„ íƒì 
                if len(content) < 500:  # ë‚´ìš©ì´ ë¶€ì¡±í•  ë•Œë§Œ
                    web_content = self._extract_medium_content(link)
                    if web_content:
                        content = web_content
                
                # Medium ì „ìš© í‚¤ì›Œë“œ í•„í„°ë§
                if not self._has_medium_keywords(title, content):
                    continue
                
                # ë‚ ì§œ íŒŒì‹±
                published_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                else:
                    published_time = datetime.now(timezone.utc)
                
                # ë‚ ì§œ í•„í„°ë§ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: ìµœê·¼ 1~2ë‹¬)
                if published_time.year < self.config.MIN_PUBLISH_YEAR:
                    logger.debug(f"{published_time.year}ë…„ ê¸°ì‚¬ ì œì™¸: {title[:50]}")
                    continue
                    
                article_age_days = (datetime.now(timezone.utc) - published_time).days
                if article_age_days > self.config.MAX_ARTICLE_AGE_DAYS:
                    logger.debug(f"{article_age_days}ì¼ ì „ ê¸°ì‚¬ ì œì™¸: {title[:50]}")
                    continue
                
                # íƒœê·¸ ì¶”ì¶œ
                tags = self._extract_medium_tags(title, content, source_id)
                
                # Medium íŠ¹í™” ì ìˆ˜ ê³„ì‚°
                score = self._calculate_medium_score(title, content, source_id)
                
                # ì‘ì„±ì ì •ë³´ (ê°€ëŠ¥í•˜ë©´)
                author = entry.get('author', 'Unknown')
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'id': f"{source_id}_{int(time.time())}_{processed_count}",
                    'title': title,
                    'title_ko': title,  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'content': content[:2500],  # Mediumì€ ì¡°ê¸ˆ ë” ê¸¸ê²Œ
                    'content_ko': content[:2500],  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'summary': '',  # ìš”ì•½ì€ ë‚˜ì¤‘ì—
                    'url': link,
                    'source': source_id,
                    'tags': tags,
                    'score': score,
                    'published': published_time.isoformat(),
                    'author': author,
                    'collected_at': datetime.now(timezone.utc).isoformat()
                }
                
                if score > 0:  # ìµœì†Œ ì ìˆ˜ í†µê³¼
                    articles.append(article_data)
                
                # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¢…ë£Œ
                if len(articles) >= limit:
                    break
                
                # Medium API ì œí•œ ë°©ì§€
                time.sleep(1)
                    
            except Exception as e:
                logger.error(f"{source_name} ê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"{source_name}ì—ì„œ {len(articles)}ê°œ ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return articles
    
    def collect_all_medium_sources(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  Medium ì†ŒìŠ¤ì—ì„œ ê¸€ ìˆ˜ì§‘"""
        all_articles = []
        
        for source_config in self.config.MEDIUM_SOURCES:
            try:
                articles = self.collect_from_medium_source(
                    source_config, 
                    limit=max(2, self.config.MAX_ARTICLES_PER_SOURCE // len(self.config.MEDIUM_SOURCES))
                )
                all_articles.extend(articles)
                
                # Medium API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"Medium ì†ŒìŠ¤ {source_config['name']} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"Mediumì—ì„œ ì´ {len(all_articles)}ê°œ ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles


if __name__ == '__main__':
    # ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸
    config = Config()
    collector = MediumCollector(config)
    
    print("ğŸ§ª Medium ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # Towards Data Science í…ŒìŠ¤íŠ¸
    test_source = {
        'name': 'Towards Data Science',
        'rss': 'https://towardsdatascience.com/feed',
        'source_id': 'towards_ds'
    }
    
    articles = collector.collect_from_medium_source(test_source, limit=3)
    print(f"\nìˆ˜ì§‘ëœ ê¸€ ìˆ˜: {len(articles)}")
    
    for i, article in enumerate(articles, 1):
        print(f"\n{i}. {article['title'][:80]}...")
        print(f"   ì ìˆ˜: {article['score']:.1f}")
        print(f"   íƒœê·¸: {', '.join(article['tags'])}")
        print(f"   ì‘ì„±ì: {article.get('author', 'Unknown')}")
        print(f"   URL: {article['url']}")
    
    print(f"\nâœ… Medium ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
