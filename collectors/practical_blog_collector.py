#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Practical Blog Collector
ì‹¤ìš© ë¸”ë¡œê·¸/ê¿€íŒ í”Œë«í¼ì—ì„œ RSS í”¼ë“œë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘
"""

import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any
import requests
import feedparser
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse

from config import Config
from collectors.content_filter import ContentFilter

logger = logging.getLogger(__name__)

class PracticalBlogCollector:
    """ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        self.content_filter = ContentFilter(config)
        
        # ìš”ì²­ í—¤ë” (Mediumì€ ë´‡ ì°¨ë‹¨ì´ ìˆì–´ì„œ ì‹¤ì œ ë¸Œë¼ìš°ì € í‰ë‚´)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, text/html, application/xhtml+xml, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ë‚ ì§œ í•„í„°ë§ ì„¤ì • (ìµœê·¼ 60ì¼)
        self.cutoff_date = datetime.now(timezone.utc) - timedelta(days=60)
        
        # ì‹¤ìš© ë¸”ë¡œê·¸ ì†ŒìŠ¤ ì„¤ì • (PRD v2.0 ê¸°ì¤€)
        self.blog_sources = {
            "towards_data_science": {
                "name": "Towards Data Science",
                "url": "https://towardsdatascience.com/feed",
                "score_bonus": 80,
                "tags": ["ë¸”ë¡œê·¸", "íŠœí† ë¦¬ì–¼", "ì‹¤ìš©"],
                "language": "en",
                "clap_filter": True  # Medium claps í•„í„°ë§
            },
            "analytics_vidhya": {
                "name": "Analytics Vidhya",
                "url": "https://www.analyticsvidhya.com/blog/feed/",
                "score_bonus": 75,
                "tags": ["ë¸”ë¡œê·¸", "ì‹¤ìŠµ", "êµìœ¡"],
                "language": "en",
                "clap_filter": False
            },
            "kdnuggets": {
                "name": "KDnuggets",
                "url": "https://www.kdnuggets.com/feed",
                "score_bonus": 75,
                "tags": ["ë¸”ë¡œê·¸", "ë¦¬ì†ŒìŠ¤", "ë‰´ìŠ¤"],
                "language": "en",
                "clap_filter": False
            },
            "neptune_ai": {
                "name": "Neptune.ai Blog",
                "url": "https://neptune.ai/blog/rss.xml",
                "score_bonus": 80,
                "tags": ["ë¸”ë¡œê·¸", "MLOps", "ê¿€íŒ"],
                "language": "en",
                "clap_filter": False
            }
        }
    
    def _fetch_rss_feed(self, rss_url: str, source_name: str) -> Optional[Any]:
        """
        RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            rss_url: RSS í”¼ë“œ URL
            source_name: ì†ŒìŠ¤ëª… (ë¡œê¹…ìš©)
            
        Returns:
            feedparser ê²°ê³¼ ê°ì²´ ë˜ëŠ” None
        """
        try:
            logger.info(f"{source_name}ì—ì„œ RSS í”¼ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {rss_url}")
            
            # íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„ ì„¤ì •
            response = self.session.get(rss_url, timeout=30)
            response.raise_for_status()
            
            # feedparserë¡œ íŒŒì‹±
            feed = feedparser.parse(response.content)
            
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"{source_name} RSS íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")
                
            logger.info(f"{source_name}ì—ì„œ {len(feed.entries)}ê°œ í•­ëª© ë°œê²¬")
            return feed
            
        except requests.exceptions.RequestException as e:
            logger.error(f"{source_name} RSS ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"{source_name} RSS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_content(self, entry: Any, source_config: Dict) -> str:
        """
        RSS í•­ëª©ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        
        Args:
            entry: feedparser í•­ëª©
            source_config: ì†ŒìŠ¤ ì„¤ì •
            
        Returns:
            ì¶”ì¶œëœ ë³¸ë¬¸ ë‚´ìš©
        """
        content = ""
        
        # summary ë˜ëŠ” contentì—ì„œ ì¶”ì¶œ
        if hasattr(entry, 'summary'):
            content = entry.summary
        elif hasattr(entry, 'content') and entry.content:
            content = entry.content[0].value if isinstance(entry.content, list) else entry.content
        elif hasattr(entry, 'description'):
            content = entry.description
        
        # HTML íƒœê·¸ ì œê±°
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            content = soup.get_text().strip()
            # ê³µë°± ì •ë¦¬
            content = re.sub(r'\s+', ' ', content)
            
        return content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
    
    def _extract_medium_claps(self, entry: Any) -> int:
        """Medium ê¸€ì˜ clap ìˆ˜ ì¶”ì¶œ (Towards Data Scienceìš©)"""
        try:
            # Medium RSSì—ì„œ clap ì •ë³´ëŠ” ë³´í†µ contentë‚˜ summaryì— í¬í•¨
            content = self._extract_content(entry, {})
            
            # clap íŒ¨í„´ ê²€ìƒ‰ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
            clap_patterns = [
                r'(\d+)\s*clap',
                r'clap[s]?\s*[\(\[](\d+)[\)\]]',
                r'ğŸ‘\s*(\d+)',
                r'(\d+)\s*ğŸ‘'
            ]
            
            for pattern in clap_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    return int(match.group(1))
            
            return 0
            
        except Exception as e:
            logger.debug(f"Clap ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0
    
    def _calculate_score(self, title: str, content: str, source_config: Dict, 
                        published_time: datetime, claps: int = 0) -> float:
        """
        PRD v2.0 ì ìˆ˜í™” ì‹œìŠ¤í…œì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚° (ë¸”ë¡œê·¸ ì „ìš©)
        
        Args:
            title: ê¸€ ì œëª©
            content: ê¸€ ë‚´ìš©
            source_config: ì†ŒìŠ¤ ì„¤ì •
            published_time: ë°œí–‰ ì‹œê°„
            claps: Medium clap ìˆ˜
            
        Returns:
            ê³„ì‚°ëœ ì ìˆ˜
        """
        # ê¸°ë³¸ ì ìˆ˜ (ë¸”ë¡œê·¸ëŠ” 80ì )
        base_score = source_config.get('score_bonus', 80)
        
        # Medium clap ë³´ë„ˆìŠ¤ (Towards Data Science ì „ìš©)
        if source_config.get('clap_filter') and claps >= 100:
            base_score += 20
            logger.debug(f"Medium clap ë³´ë„ˆìŠ¤ +20 ({claps} claps): {title[:50]}")
        
        # ì‹¤ìš© ê°€ì´ë“œ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+20ì )
        guide_keywords = ["how to", "guide", "tutorial", "walkthrough", "step-by-step", 
                         "implementation", "practical", "hands-on"]
        if any(keyword.lower() in title.lower() for keyword in guide_keywords):
            base_score += 20
            logger.debug(f"ì‹¤ìš© ê°€ì´ë“œ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ +20: {title[:50]}")
        
        # ì‹¤ë¬´ ì‚¬ë¡€ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+15ì )
        case_keywords = ["case study", "experience", "lessons learned", "how we", 
                        "real-world", "production", "in practice"]
        if any(keyword.lower() in title.lower() for keyword in case_keywords):
            base_score += 15
            logger.debug(f"ì‹¤ë¬´ ì‚¬ë¡€ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ +15: {title[:50]}")
        
        # LLM ê´€ë ¨ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+10ì )
        llm_keywords = ["llm", "gpt", "transformer", "language model", "claude", "gemini", 
                       "chatgpt", "openai", "bert", "t5"]
        if any(keyword.lower() in title.lower() or keyword.lower() in content.lower() for keyword in llm_keywords):
            base_score += 10
            logger.debug(f"LLM í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ +10: {title[:50]}")
        
        # ì‹œê³„ì—´ ê´€ë ¨ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+10ì )
        timeseries_keywords = ["time series", "forecasting", "prediction", "forecast", "arima", "lstm"]
        if any(keyword.lower() in title.lower() or keyword.lower() in content.lower() for keyword in timeseries_keywords):
            base_score += 10
            logger.debug(f"ì‹œê³„ì—´ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ +10: {title[:50]}")
        
        # MLOps í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (+10ì )
        mlops_keywords = ["mlops", "ml ops", "deployment", "monitoring", "pipeline", "kubernetes", 
                         "docker", "model serving", "experiment tracking"]
        if any(keyword.lower() in title.lower() or keyword.lower() in content.lower() for keyword in mlops_keywords):
            base_score += 10
            logger.debug(f"MLOps í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ +10: {title[:50]}")
        
        # ì‹ ì„ ë„ ë³´ë„ˆìŠ¤
        hours_old = (datetime.now(timezone.utc) - published_time).total_seconds() / 3600
        if hours_old < 24:
            base_score += 10
            logger.debug(f"24ì‹œê°„ ì´ë‚´ ì‹ ì„ ë„ ë³´ë„ˆìŠ¤ +10: {title[:50]}")
        elif hours_old < 168:  # 1ì£¼ì¼
            base_score += 5
            logger.debug(f"1ì£¼ì¼ ì´ë‚´ ì‹ ì„ ë„ ë³´ë„ˆìŠ¤ +5: {title[:50]}")
        
        # íŒ¨ë„í‹°
        opinion_keywords = ["opinion", "thoughts on", "my take", "commentary", "i think", "personal"]
        if any(keyword.lower() in title.lower() for keyword in opinion_keywords):
            base_score -= 20
            logger.debug(f"ì˜ê²¬ ê¸°ì‚¬ íŒ¨ë„í‹° -20: {title[:50]}")
        
        question_keywords = ["what do you think", "recommendations?", "suggestions?", 
                           "help me", "which should i", "what should i"]
        if any(keyword.lower() in title.lower() for keyword in question_keywords):
            base_score -= 30
            logger.debug(f"ë‹¨ìˆœ ì§ˆë¬¸ íŒ¨ë„í‹° -30: {title[:50]}")
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ë„í‹°
        if len(content) < 500:
            base_score -= 15
            logger.debug(f"ì§§ì€ ê¸€ íŒ¨ë„í‹° -15: {title[:50]}")
        
        return base_score
    
    def _parse_published_time(self, entry: Any) -> datetime:
        """RSS í•­ëª©ì—ì„œ ë°œí–‰ ì‹œê°„ íŒŒì‹±"""
        published_time = datetime.now(timezone.utc)
        
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            published_time = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            published_time = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
        elif hasattr(entry, 'published'):
            try:
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                import dateutil.parser
                published_time = dateutil.parser.parse(entry.published)
                if published_time.tzinfo is None:
                    published_time = published_time.replace(tzinfo=timezone.utc)
            except:
                pass
                
        return published_time
    
    def collect(self, max_articles_per_source: int = 8) -> List[Dict[str, Any]]:
        """
        ì‹¤ìš© ë¸”ë¡œê·¸ì—ì„œ ê¸€ ìˆ˜ì§‘
        
        Args:
            max_articles_per_source: ì†ŒìŠ¤ë³„ ìµœëŒ€ ìˆ˜ì§‘ ê¸€ ìˆ˜
            
        Returns:
            ìˆ˜ì§‘ëœ ê¸€ ëª©ë¡
        """
        all_articles = []
        
        logger.info("ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹œì‘")
        
        for source_id, source_config in self.blog_sources.items():
            try:
                logger.info(f"{source_config['name']} ìˆ˜ì§‘ ì¤‘...")
                
                # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
                feed = self._fetch_rss_feed(source_config['url'], source_config['name'])
                if not feed or not hasattr(feed, 'entries'):
                    continue
                
                source_articles = []
                
                for entry in feed.entries:
                    try:
                        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        title = entry.get('title', '').strip()
                        if not title:
                            continue
                            
                        url = entry.get('link', '').strip()
                        if not url:
                            continue
                        
                        # ë°œí–‰ ì‹œê°„ íŒŒì‹±
                        published_time = self._parse_published_time(entry)
                        
                        # ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ 60ì¼)
                        if published_time < self.cutoff_date:
                            continue
                        
                        # 2025ë…„ ì´ì „ ê¸€ ì œì™¸
                        if published_time.year < 2025:
                            continue
                        
                        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
                        content = self._extract_content(entry, source_config)
                        
                        # Medium clap ìˆ˜ ì¶”ì¶œ (í•„ìš”ì‹œ)
                        claps = 0
                        if source_config.get('clap_filter'):
                            claps = self._extract_medium_claps(entry)
                            # clapì´ ë„ˆë¬´ ì ìœ¼ë©´ ì œì™¸ (Towards Data Science)
                            if claps < 50 and claps > 0:
                                logger.debug(f"clap ë¶€ì¡±ìœ¼ë¡œ ì œì™¸ ({claps} claps): {title[:50]}")
                                continue
                        
                        # AI/ML/DS ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                        ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'ml', 'deep learning', 
                                     'data science', 'llm', 'gpt', 'neural network', 'python', 'tensorflow',
                                     'pytorch', 'pandas', 'sklearn', 'nlp', 'computer vision', 'statistics']
                        
                        title_content_combined = (title + ' ' + content).lower()
                        if not any(keyword.lower() in title_content_combined for keyword in ai_keywords):
                            logger.debug(f"AI/ML/DS í‚¤ì›Œë“œ ì—†ìŒìœ¼ë¡œ ì œì™¸: {title[:50]}")
                            continue
                        
                        # ì ìˆ˜ ê³„ì‚°
                        score = self._calculate_score(title, content, source_config, published_time, claps)
                        
                        # ìµœì†Œ ì ìˆ˜ í•„í„°ë§ (ë¸”ë¡œê·¸ëŠ” 70ì  ì´ìƒ)
                        if score < 70:
                            logger.debug(f"ì ìˆ˜ ë¶€ì¡±ìœ¼ë¡œ ì œì™¸ ({score}ì ): {title[:50]}")
                            continue
                        
                        # ê¸€ ì •ë³´ êµ¬ì„±
                        article = {
                            'id': f"{source_id}_{hash(url) % 1000000}",
                            'title': title,
                            'content': content,
                            'url': url,
                            'source': source_config['name'],
                            'source_id': source_id,
                            'tags': source_config['tags'].copy(),
                            'score': score,
                            'published': published_time.isoformat(),
                            'collected_at': datetime.now(timezone.utc).isoformat(),
                            'claps': claps if claps > 0 else None,
                            'needs_translation': source_config.get('language', 'en') == 'en'  # ì˜ì–´ ê¸€ë§Œ ë²ˆì—­ í•„ìš”
                        }
                        
                        source_articles.append(article)
                        logger.info(f"ìˆ˜ì§‘ ì„±ê³µ ({score}ì ): {title[:80]}")
                        
                    except Exception as e:
                        logger.error(f"ê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì„ íƒ
                source_articles.sort(key=lambda x: x['score'], reverse=True)
                selected_articles = source_articles[:max_articles_per_source]
                
                all_articles.extend(selected_articles)
                logger.info(f"{source_config['name']}: {len(selected_articles)}ê°œ ê¸€ ì„ íƒ")
                
                # API í˜¸ì¶œ ê°„ê²© (Rate Limiting ë°©ì§€)
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"{source_config['name']} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸€")
        return all_articles

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    collector = PracticalBlogCollector()
    articles = collector.collect(max_articles_per_source=3)
    
    print(f"\nì´ {len(articles)}ê°œ ë¸”ë¡œê·¸ ê¸€ ìˆ˜ì§‘ë¨")
    for article in articles[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
        print(f"- {article['source']}: {article['title'][:60]} (ì ìˆ˜: {article['score']})")
