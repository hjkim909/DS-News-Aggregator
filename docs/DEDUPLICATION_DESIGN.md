# ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±° ì‹œìŠ¤í…œ ì„¤ê³„

> **ëª©í‘œ:** URL ê¸°ë°˜ ì¤‘ë³µ ì œê±°ë¡œ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë° ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

---

## ğŸ“‹ ëª©ì°¨

1. [ë¬¸ì œ ì •ì˜](#ë¬¸ì œ-ì •ì˜)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [Phase 1: URL í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°](#phase-1-url-í•´ì‹œ-ê¸°ë°˜-ì¤‘ë³µ-ì œê±°)
4. [Phase 2: ê³ ê¸‰ ì¤‘ë³µ ê°ì§€](#phase-2-ê³ ê¸‰-ì¤‘ë³µ-ê°ì§€)
5. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)
6. [í…ŒìŠ¤íŠ¸ ê³„íš](#í…ŒìŠ¤íŠ¸-ê³„íš)

---

## ğŸ¯ ë¬¸ì œ ì •ì˜

### í˜„ì¬ ë¬¸ì œì 

1. **ë™ì¼ URL ì¤‘ë³µ**
   - ê°™ì€ ë‰´ìŠ¤ê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ì§‘ë  ìˆ˜ ìˆìŒ
   - ë‚ ì§œë³„ íŒŒì¼ ê°„ ì¤‘ë³µ ì²´í¬ ì—†ìŒ

2. **ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ê°™ì€ ë‰´ìŠ¤**
   - TechCrunchì™€ WIREDê°€ ê°™ì€ ë‰´ìŠ¤ë¥¼ ë‹¤ë¥¸ URLë¡œ ê²Œì¬
   - ì œëª©ë§Œ ì‚´ì§ ë‹¤ë¥´ê³  ë‚´ìš©ì€ ë™ì¼

3. **ì‚¬ìš©ì ê²½í—˜ ì €í•˜**
   - ì´ë¯¸ ë³¸ ê¸€ì´ ë‹¤ì‹œ ë‚˜íƒ€ë‚¨
   - ì‹¤ì œ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì ìŒ

### ëª©í‘œ

- âœ… ì¤‘ë³µ ë‰´ìŠ¤ 95% ì´ìƒ ì œê±°
- âœ… ë°ì´í„° í’ˆì§ˆ 30% í–¥ìƒ
- âœ… ì €ì¥ ê³µê°„ 30% ì ˆì•½
- âœ… ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: ê¸€ ìˆ˜ì§‘ (TechCrunch, MIT, OpenAI, etc.)            â”‚
â”‚  - 32ê°œ ìˆ˜ì§‘                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1.5: ì¤‘ë³µ ì œê±° (NEW!) ğŸ”¥                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ArticleDeduplicator                                   â”‚  â”‚
â”‚  â”‚ - URL í•´ì‹œ ì²´í¬                                        â”‚  â”‚
â”‚  â”‚ - íˆìŠ¤í† ë¦¬ íŒŒì¼ í™•ì¸                                    â”‚  â”‚
â”‚  â”‚ - ì œëª© ìœ ì‚¬ë„ ë¶„ì„ (Phase 2)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  - 32ê°œ â†’ 24ê°œ (8ê°œ ì¤‘ë³µ ì œê±°)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: ì½˜í…ì¸  í•„í„°ë§                                        â”‚
â”‚  - 24ê°œ â†’ 9ê°œ                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3-5: ë²ˆì—­ â†’ ìš”ì•½ â†’ ì €ì¥                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¨ Phase 1: URL í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

### 1.1 ArticleDeduplicator í´ë˜ìŠ¤

**íŒŒì¼:** `processors/deduplicator.py`

```python
import hashlib
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Set

class ArticleDeduplicator:
    """
    URL í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ
    """
    
    def __init__(self, history_file: str = 'data/.dedup_history.json', 
                 retention_days: int = 30):
        """
        Args:
            history_file: íˆìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ
            retention_days: íˆìŠ¤í† ë¦¬ ë³´ê´€ ê¸°ê°„ (ì¼)
        """
        self.history_file = history_file
        self.retention_days = retention_days
        self.url_hashes: Set[str] = set()
        self.stats = {
            'total_checked': 0,
            'duplicates_found': 0,
            'unique_articles': 0
        }
        
        # íˆìŠ¤í† ë¦¬ ë¡œë“œ
        self._load_history()
    
    def _hash_url(self, url: str) -> str:
        """URLì„ SHA256 í•´ì‹œë¡œ ë³€í™˜"""
        return hashlib.sha256(url.encode('utf-8')).hexdigest()
    
    def _load_history(self):
        """íˆìŠ¤í† ë¦¬ íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r') as f:
                    data = json.load(f)
                    
                # 30ì¼ ì´ë‚´ URLë§Œ ë¡œë“œ
                cutoff_date = datetime.now() - timedelta(days=self.retention_days)
                
                for entry in data.get('urls', []):
                    collected_date = datetime.fromisoformat(entry['collected_at'])
                    if collected_date > cutoff_date:
                        self.url_hashes.add(entry['hash'])
                        
            except Exception as e:
                print(f"íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.url_hashes = set()
        else:
            self.url_hashes = set()
    
    def _save_history(self):
        """íˆìŠ¤í† ë¦¬ íŒŒì¼ ì €ì¥"""
        # ìµœì‹  30ì¼ ë°ì´í„°ë§Œ ì €ì¥
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        
        urls_data = []
        for url_hash in self.url_hashes:
            urls_data.append({
                'hash': url_hash,
                'collected_at': datetime.now().isoformat()
            })
        
        data = {
            'last_updated': datetime.now().isoformat(),
            'retention_days': self.retention_days,
            'total_urls': len(urls_data),
            'urls': urls_data
        }
        
        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
        
        with open(self.history_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def is_duplicate(self, article: Dict[str, Any]) -> bool:
        """
        URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        
        Args:
            article: ì²´í¬í•  ê¸°ì‚¬
            
        Returns:
            True if duplicate, False otherwise
        """
        url = article.get('url', '')
        if not url:
            return False
        
        url_hash = self._hash_url(url)
        return url_hash in self.url_hashes
    
    def add_article(self, article: Dict[str, Any]):
        """URL í•´ì‹œ ì¶”ê°€"""
        url = article.get('url', '')
        if url:
            url_hash = self._hash_url(url)
            self.url_hashes.add(url_hash)
    
    def remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì¤‘ë³µ ì œê±° ë©”ì¸ ë©”ì„œë“œ
        
        Args:
            articles: ì²´í¬í•  ê¸°ì‚¬ ëª©ë¡
            
        Returns:
            ì¤‘ë³µì´ ì œê±°ëœ ê¸°ì‚¬ ëª©ë¡
        """
        unique_articles = []
        
        for article in articles:
            self.stats['total_checked'] += 1
            
            if not self.is_duplicate(article):
                unique_articles.append(article)
                self.add_article(article)
                self.stats['unique_articles'] += 1
            else:
                self.stats['duplicates_found'] += 1
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self._save_history()
        
        return unique_articles
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° í†µê³„ ë°˜í™˜"""
        return {
            'total_checked': self.stats['total_checked'],
            'duplicates_found': self.stats['duplicates_found'],
            'unique_articles': self.stats['unique_articles'],
            'duplicate_rate': f"{self.stats['duplicates_found'] / max(self.stats['total_checked'], 1) * 100:.1f}%",
            'history_size': len(self.url_hashes)
        }
```

### 1.2 íŒŒì´í”„ë¼ì¸ í†µí•©

**íŒŒì¼:** `processors/pipeline.py`

```python
from processors.deduplicator import ArticleDeduplicator

class DSNewsPipeline:
    def __init__(self, config: Config = None):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ ì¶”ê°€
        self.deduplicator = ArticleDeduplicator()
    
    def step1_5_remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        1.5ë‹¨ê³„: ì¤‘ë³µ ì œê±° (NEW!)
        """
        self._log_stage_start("ì¤‘ë³µ ì œê±°")
        
        try:
            unique_articles = self.deduplicator.remove_duplicates(articles)
            
            # í†µê³„ ì¶œë ¥
            stats = self.deduplicator.get_stats()
            logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {stats}")
            logger.info(f"  - ì „ì²´: {stats['total_checked']}ê°œ")
            logger.info(f"  - ì¤‘ë³µ: {stats['duplicates_found']}ê°œ ({stats['duplicate_rate']})")
            logger.info(f"  - ê³ ìœ : {stats['unique_articles']}ê°œ")
            
            self.pipeline_stats['duplicates_removed'] = stats['duplicates_found']
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
            unique_articles = articles  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
        self._log_stage_end("ì¤‘ë³µ ì œê±°", len(unique_articles))
        return unique_articles
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # 1ë‹¨ê³„: ê¸€ ìˆ˜ì§‘
        articles = self.step1_collect_articles()
        
        # 1.5ë‹¨ê³„: ì¤‘ë³µ ì œê±° (NEW!)
        articles = self.step1_5_remove_duplicates(articles)
        
        # 2ë‹¨ê³„: ì½˜í…ì¸  í•„í„°ë§
        articles = self.step2_filter_articles(articles)
        
        # ... ë‚˜ë¨¸ì§€ ë‹¨ê³„ ...
```

### 1.3 íˆìŠ¤í† ë¦¬ íŒŒì¼ êµ¬ì¡°

**íŒŒì¼:** `data/.dedup_history.json`

```json
{
  "last_updated": "2025-10-13T18:00:00",
  "retention_days": 30,
  "total_urls": 245,
  "urls": [
    {
      "hash": "a3f5d9e8c2b1...",
      "collected_at": "2025-10-13T10:30:00"
    },
    {
      "hash": "b2e4c7a9f3d8...",
      "collected_at": "2025-10-12T08:15:00"
    }
  ]
}
```

---

## ğŸš€ Phase 2: ê³ ê¸‰ ì¤‘ë³µ ê°ì§€

### 2.1 ì œëª© ìœ ì‚¬ë„ ë¶„ì„

**ë¼ì´ë¸ŒëŸ¬ë¦¬:** `python-Levenshtein`

```python
from Levenshtein import ratio

class AdvancedDeduplicator(ArticleDeduplicator):
    """ê³ ê¸‰ ì¤‘ë³µ ê°ì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, similarity_threshold: float = 0.85, **kwargs):
        super().__init__(**kwargs)
        self.similarity_threshold = similarity_threshold
        self.title_cache: List[str] = []
    
    def _title_similarity(self, title1: str, title2: str) -> float:
        """
        ë‘ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Returns:
            0.0 ~ 1.0 (1.0 = ì™„ì „ ë™ì¼)
        """
        # ì†Œë¬¸ì ë³€í™˜ ë° ê³µë°± ì •ê·œí™”
        t1 = ' '.join(title1.lower().split())
        t2 = ' '.join(title2.lower().split())
        
        # Levenshtein distance ê¸°ë°˜ ìœ ì‚¬ë„
        return ratio(t1, t2)
    
    def is_duplicate_advanced(self, article: Dict[str, Any]) -> bool:
        """
        URL + ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        """
        # 1. URL ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´)
        if self.is_duplicate(article):
            return True
        
        # 2. ì œëª© ìœ ì‚¬ë„ ì²´í¬ (ì‹ ê·œ)
        current_title = article.get('title', '')
        if not current_title:
            return False
        
        for cached_title in self.title_cache:
            similarity = self._title_similarity(current_title, cached_title)
            
            if similarity >= self.similarity_threshold:
                # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                logger.info(f"ì œëª© ìœ ì‚¬ë„ ì¤‘ë³µ ê°ì§€: {similarity*100:.1f}%")
                logger.info(f"  - ê¸°ì¡´: {cached_title[:50]}...")
                logger.info(f"  - ì‹ ê·œ: {current_title[:50]}...")
                return True
        
        return False
    
    def remove_duplicates(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° (ê³ ê¸‰)"""
        unique_articles = []
        
        for article in articles:
            if not self.is_duplicate_advanced(article):
                unique_articles.append(article)
                self.add_article(article)
                
                # ì œëª© ìºì‹œ ì¶”ê°€
                title = article.get('title', '')
                if title:
                    self.title_cache.append(title)
            
        self._save_history()
        return unique_articles
```

### 2.2 ì½˜í…ì¸  í•‘ê±°í”„ë¦°íŒ…

```python
def _content_fingerprint(self, content: str, n: int = 100) -> str:
    """
    ì½˜í…ì¸ ì˜ ì²˜ìŒ nê°œ ë‹¨ì–´ë¡œ í•‘ê±°í”„ë¦°íŠ¸ ìƒì„±
    """
    words = content.lower().split()[:n]
    text = ' '.join(words)
    return hashlib.sha256(text.encode('utf-8')).hexdigest()
```

---

## ğŸ“… êµ¬í˜„ ê³„íš

### Week 1: Phase 1 êµ¬í˜„

**Day 1-2: ArticleDeduplicator í´ë˜ìŠ¤**
- [ ] `processors/deduplicator.py` ìƒì„±
- [ ] URL í•´ì‹œ ë©”ì„œë“œ êµ¬í˜„
- [ ] íˆìŠ¤í† ë¦¬ ë¡œë“œ/ì €ì¥ êµ¬í˜„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**Day 3-4: íŒŒì´í”„ë¼ì¸ í†µí•©**
- [ ] `pipeline.py`ì— `step1_5_remove_duplicates()` ì¶”ê°€
- [ ] ë¡œê¹… ë° í†µê³„ ì¶”ê°€
- [ ] í†µí•© í…ŒìŠ¤íŠ¸

**Day 5: íˆìŠ¤í† ë¦¬ ê´€ë¦¬**
- [ ] 30ì¼ ìë™ ì •ë¦¬ êµ¬í˜„
- [ ] íŒŒì¼ í¬ê¸° ì œí•œ
- [ ] `.gitignore`ì— íˆìŠ¤í† ë¦¬ íŒŒì¼ ì¶”ê°€

### Week 2-3: Phase 2 êµ¬í˜„

**Week 2: ì œëª© ìœ ì‚¬ë„**
- [ ] `python-Levenshtein` ì„¤ì¹˜
- [ ] `AdvancedDeduplicator` í´ë˜ìŠ¤ êµ¬í˜„
- [ ] ì œëª© ìºì‹œ ê´€ë¦¬
- [ ] ìœ ì‚¬ë„ ì„ê³„ê°’ íŠœë‹

**Week 3: ì½˜í…ì¸  í•‘ê±°í”„ë¦°íŒ…**
- [ ] ì½˜í…ì¸  í•´ì‹œ ë©”ì„œë“œ êµ¬í˜„
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/test_deduplicator.py

def test_url_hash():
    """URL í•´ì‹œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    dedup = ArticleDeduplicator()
    hash1 = dedup._hash_url("https://example.com/article1")
    hash2 = dedup._hash_url("https://example.com/article1")
    hash3 = dedup._hash_url("https://example.com/article2")
    
    assert hash1 == hash2
    assert hash1 != hash3

def test_duplicate_detection():
    """ì¤‘ë³µ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    dedup = ArticleDeduplicator()
    
    article1 = {'url': 'https://example.com/1', 'title': 'Test'}
    article2 = {'url': 'https://example.com/1', 'title': 'Test'}
    
    assert not dedup.is_duplicate(article1)
    dedup.add_article(article1)
    assert dedup.is_duplicate(article2)

def test_title_similarity():
    """ì œëª© ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸"""
    dedup = AdvancedDeduplicator()
    
    title1 = "OpenAI releases GPT-5"
    title2 = "OpenAI Releases GPT-5"
    title3 = "Google releases Gemini 2.0"
    
    sim1 = dedup._title_similarity(title1, title2)
    sim2 = dedup._title_similarity(title1, title3)
    
    assert sim1 > 0.95  # ê±°ì˜ ë™ì¼
    assert sim2 < 0.5   # ë§¤ìš° ë‹¤ë¦„
```

### í†µí•© í…ŒìŠ¤íŠ¸

```python
def test_pipeline_integration():
    """íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    pipeline = DSNewsPipeline()
    
    # ì¤‘ë³µ í¬í•¨ ê¸°ì‚¬ ìˆ˜ì§‘
    articles = [
        {'url': 'https://example.com/1', 'title': 'Article 1'},
        {'url': 'https://example.com/1', 'title': 'Article 1'},  # ì¤‘ë³µ
        {'url': 'https://example.com/2', 'title': 'Article 2'},
    ]
    
    unique = pipeline.step1_5_remove_duplicates(articles)
    
    assert len(unique) == 2
    assert pipeline.deduplicator.stats['duplicates_found'] == 1
```

---

## ğŸ“Š ì„±ê³µ ì§€í‘œ

### Phase 1 ëª©í‘œ
- âœ… URL ì¤‘ë³µ 100% ì œê±°
- âœ… íˆìŠ¤í† ë¦¬ íŒŒì¼ í¬ê¸° < 1MB
- âœ… ì¤‘ë³µ ì œê±° ì†ë„ < 100ms (100ê°œ ê¸°ì‚¬ ê¸°ì¤€)

### Phase 2 ëª©í‘œ
- âœ… ì œëª© ìœ ì‚¬ ì¤‘ë³µ 80% ì´ìƒ ê°ì§€
- âœ… False Positive < 5%
- âœ… ì „ì²´ ì¤‘ë³µ ì œê±°ìœ¨ 95% ì´ìƒ

---

## ğŸ”§ ì„¤ì • ì˜µì…˜

**config.pyì— ì¶”ê°€:**

```python
# ì¤‘ë³µ ì œê±° ì„¤ì •
DEDUP_ENABLED = True
DEDUP_HISTORY_FILE = 'data/.dedup_history.json'
DEDUP_RETENTION_DAYS = 30
DEDUP_TITLE_SIMILARITY_THRESHOLD = 0.85
DEDUP_USE_ADVANCED = False  # Phase 2 ê¸°ëŠ¥
```

---

## ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

### ê¸°ì¡´ ë°ì´í„° ì²˜ë¦¬

1. **íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”**
   ```bash
   python -c "from processors.deduplicator import ArticleDeduplicator; 
              d = ArticleDeduplicator(); 
              d._save_history()"
   ```

2. **ê¸°ì¡´ ê¸€ URL ì¶”ê°€**
   ```python
   # ìµœê·¼ 30ì¼ ë°ì´í„°ì—ì„œ URL ì¶”ì¶œ
   for date_file in recent_files:
       articles = load_articles(date_file)
       for article in articles:
           dedup.add_article(article)
   ```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

Phase 1 ì™„ë£Œ í›„:
1. Phase 2 êµ¬í˜„ ê²€í† 
2. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
4. í•„ìš”ì‹œ ì¶”ê°€ ìµœì í™”

---

**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025-10-13
**ë‹´ë‹¹ì:** hjkim909
**ìš°ì„ ìˆœìœ„:** URGENT ğŸ”¥

