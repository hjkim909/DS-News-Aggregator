#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Hacker News Collector
Hacker News APIë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘
"""

import logging
import time
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any
import requests
from bs4 import BeautifulSoup
import re

from config import Config
from collectors.content_filter import ContentFilter

logger = logging.getLogger(__name__)

class HackerNewsCollector:
    """Hacker News ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        Hacker News ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        self.content_filter = ContentFilter(config)
        
        # Hacker News API ì„¤ì •
        self.base_api_url = self.config.HACKER_NEWS_CONFIG['base_url']
        self.web_base_url = self.config.HACKER_NEWS_CONFIG['web_url']
        
        # ìš”ì²­ í—¤ë”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'application/json, text/html, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'DNT': '1',
            'Connection': 'keep-alive'
        }
        
        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Hacker Newsì—ì„œ ì°¾ì„ DS/ML í‚¤ì›Œë“œ
        self.hn_keywords = [
            # AI/ML í•µì‹¬
            'machine learning', 'ml', 'deep learning', 'neural network',
            'artificial intelligence', 'ai', 'data science', 'data analysis',
            
            # ìµœì‹  íŠ¸ë Œë“œ  
            'llm', 'gpt', 'chatgpt', 'openai', 'anthropic', 'claude',
            'transformer', 'bert', 'stable diffusion', 'generative ai',
            
            # ê¸°ìˆ /ë„êµ¬
            'tensorflow', 'pytorch', 'scikit-learn', 'jupyter', 'pandas',
            'python data', 'r programming', 'kaggle', 'colab',
            
            # ì‘ìš©ë¶„ì•¼
            'computer vision', 'nlp', 'natural language', 'recommendation',
            'time series', 'forecasting', 'autonomous', 'robotics',
            
            # ì—…ê³„/íšŒì‚¬
            'deepmind', 'google ai', 'facebook ai', 'microsoft ai',
            'nvidia', 'tesla', 'uber ai', 'netflix data'
        ]
    
    def _get_top_story_ids(self, story_type: str = 'topstories', limit: int = 100) -> List[int]:
        """ìƒìœ„ ìŠ¤í† ë¦¬ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = f"{self.base_api_url}/{story_type}.json"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            story_ids = response.json()
            return story_ids[:limit] if story_ids else []
            
        except Exception as e:
            logger.error(f"Hacker News {story_type} ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def _get_story_details(self, story_id: int) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ìŠ¤í† ë¦¬ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = f"{self.base_api_url}/item/{story_id}.json"
            response = self.session.get(url, timeout=5)
            response.raise_for_status()
            
            story_data = response.json()
            
            # ê¸°ë³¸ ê²€ì¦
            if not story_data or story_data.get('type') != 'story':
                return None
            
            if story_data.get('deleted') or story_data.get('dead'):
                return None
                
            return story_data
            
        except Exception as e:
            logger.warning(f"ìŠ¤í† ë¦¬ ID {story_id} ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_content_from_url(self, url: str) -> str:
        """ë§í¬ëœ ì›¹í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ ì„ íƒì
            content_selectors = [
                'article', 'main', '[role="main"]',
                '.content', '.post-content', '.entry-content',
                '.article-content', '#content', '.post-body'
            ]
            
            content_text = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                        for unwanted in elem(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                            unwanted.decompose()
                        
                        text = elem.get_text(separator=' ', strip=True)
                        if text and len(text) > 100:
                            content_text = text
                            break
                    if content_text:
                        break
            
            # ë°±ì—…: bodyì—ì„œ ì¶”ì¶œ
            if not content_text:
                body = soup.find('body')
                if body:
                    for tag in body(['script', 'style', 'nav', 'footer', 'header']):
                        tag.decompose()
                    content_text = body.get_text(separator=' ', strip=True)
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            content_text = re.sub(r'\s+', ' ', content_text).strip()
            
            # ìµœëŒ€ 1500ìë¡œ ì œí•œ (Hacker NewsëŠ” ë³´í†µ ì§§ì€ ê¸€ ìœ„ì£¼)
            return content_text[:1500] if len(content_text) > 1500 else content_text
            
        except Exception as e:
            logger.warning(f"URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ""
    
    def _extract_hn_tags(self, title: str, content: str, url: str = "") -> List[str]:
        """Hacker News ê¸€ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = set(['Hacker News'])  # ê¸°ë³¸ íƒœê·¸
        text = (title + " " + content + " " + url).lower()
        
        # AI/ML ê´€ë ¨ íƒœê·¸
        ai_tags = {
            'machine learning': 'Machine Learning',
            'deep learning': 'Deep Learning', 
            'artificial intelligence': 'AI',
            'data science': 'Data Science',
            'neural network': 'Neural Networks',
            'computer vision': 'Computer Vision',
            'natural language': 'NLP',
            'time series': 'Time Series'
        }
        
        for keyword, tag in ai_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ìµœì‹  AI ëª¨ë¸/ì„œë¹„ìŠ¤
        model_tags = {
            'gpt': 'GPT', 'chatgpt': 'ChatGPT', 'openai': 'OpenAI',
            'claude': 'Claude', 'anthropic': 'Anthropic',
            'llm': 'LLM', 'transformer': 'Transformers',
            'stable diffusion': 'Stable Diffusion',
            'midjourney': 'Midjourney'
        }
        
        for keyword, tag in model_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ê¸°ìˆ  íšŒì‚¬
        company_tags = {
            'google': 'Google', 'deepmind': 'DeepMind',
            'microsoft': 'Microsoft', 'facebook': 'Meta', 'meta': 'Meta',
            'apple': 'Apple', 'nvidia': 'NVIDIA', 'tesla': 'Tesla',
            'uber': 'Uber', 'netflix': 'Netflix'
        }
        
        for keyword, tag in company_tags.items():
            if keyword in text:
                tags.add(tag)
        
        # ê¸°ìˆ  ìŠ¤íƒ
        tech_tags = {
            'python': 'Python', 'javascript': 'JavaScript', 'rust': 'Rust',
            'tensorflow': 'TensorFlow', 'pytorch': 'PyTorch',
            'docker': 'Docker', 'kubernetes': 'Kubernetes'
        }
        
        for keyword, tag in tech_tags.items():
            if keyword in text:
                tags.add(tag)
        
        return list(tags)
    
    def _calculate_hn_score(self, title: str, content: str, score: int, comments: int, url: str = "") -> float:
        """Hacker News ê¸€ íŠ¹í™” ì ìˆ˜ ê³„ì‚°"""
        base_score = self.content_filter.calculate_score(title, content, 'hackernews')
        
        # Hacker News íŠ¹í™” ë³´ë„ˆìŠ¤
        
        # ì ìˆ˜(ì—…ë³´íŠ¸) ë³´ë„ˆìŠ¤: ë†’ì€ ì ìˆ˜ì¼ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ìŒ
        if score >= 100:
            base_score += 20
        elif score >= 50:
            base_score += 15
        elif score >= 20:
            base_score += 10
        elif score >= 10:
            base_score += 5
        
        # ëŒ“ê¸€ ìˆ˜ ë³´ë„ˆìŠ¤: í™œë°œí•œ í† ë¡ ì´ ìˆëŠ” ê¸€
        if comments >= 50:
            base_score += 15
        elif comments >= 20:
            base_score += 10
        elif comments >= 10:
            base_score += 5
        
        text = (title + " " + content + " " + url).lower()
        
        # íŠ¹ë³„ ë„ë©”ì¸ ë³´ë„ˆìŠ¤
        premium_domains = [
            'arxiv.org', 'nature.com', 'science.org', 'ieee.org',
            'acm.org', 'mit.edu', 'stanford.edu', 'berkeley.edu',
            'deepmind.com', 'openai.com', 'anthropic.com',
            'blog.google', 'research.google', 'ai.googleblog.com'
        ]
        
        for domain in premium_domains:
            if domain in url:
                base_score += 15
                break
        
        # ì—°êµ¬/ë…¼ë¬¸ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
        research_keywords = ['paper', 'research', 'study', 'analysis', 'arxiv', 'doi']
        for keyword in research_keywords:
            if keyword in text:
                base_score += 10
                break
        
        return base_score
    
    def _has_hn_keywords(self, title: str, content: str, url: str = "") -> bool:
        """Hacker News DS/ML í‚¤ì›Œë“œ í•„í„°ë§"""
        text = (title + " " + content + " " + url).lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in self.hn_keywords:
            if keyword in text:
                return True
        
        # URL ê¸°ë°˜ ë§¤ì¹­ (íŠ¹ë³„ ë„ë©”ì¸)
        ai_domains = [
            'arxiv.org', 'ai.googleblog.com', 'openai.com', 'deepmind.com',
            'research.google', 'ai.facebook.com', 'microsoft.com/ai',
            'nvidia.com/ai', 'anthropic.com'
        ]
        
        for domain in ai_domains:
            if domain in url:
                return True
        
        return False
    
    def collect_from_hackernews(self, limit: int = 15) -> List[Dict[str, Any]]:
        """Hacker Newsì—ì„œ ê¸€ ìˆ˜ì§‘"""
        logger.info(f"Hacker Newsì—ì„œ ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        
        # 1. ìƒìœ„ ìŠ¤í† ë¦¬ ID ê°€ì ¸ì˜¤ê¸° (ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§)
        story_ids = self._get_top_story_ids('topstories', limit * 10)
        if not story_ids:
            logger.warning("Hacker News ìŠ¤í† ë¦¬ IDë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        articles = []
        processed_count = 0
        
        for story_id in story_ids:
            try:
                processed_count += 1
                
                # 2. ìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                story_data = self._get_story_details(story_id)
                if not story_data:
                    continue
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = story_data.get('title', '').strip()
                if not title:
                    continue
                
                url = story_data.get('url', '')
                hn_score = story_data.get('score', 0)
                comments_count = story_data.get('descendants', 0)
                timestamp = story_data.get('time', time.time())
                author = story_data.get('by', 'anonymous')
                
                # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° (Ask HN, Show HN ë“±)
                text_content = story_data.get('text', '')
                if text_content:
                    # HTML íƒœê·¸ ì œê±°
                    content = BeautifulSoup(text_content, 'html.parser').get_text(strip=True)
                else:
                    content = ""
                
                # ì™¸ë¶€ ë§í¬ê°€ ìˆìœ¼ë©´ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                if url and not content:
                    web_content = self._extract_content_from_url(url)
                    if web_content:
                        content = web_content
                
                # DS/ML í‚¤ì›Œë“œ í•„í„°ë§
                if not self._has_hn_keywords(title, content, url):
                    continue
                
                # ìµœì†Œ í’ˆì§ˆ í•„í„°ë§ (ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸)
                if hn_score < 5:
                    continue
                
                # ë‚ ì§œ ë³€í™˜
                published_time = datetime.fromtimestamp(timestamp, timezone.utc)
                
                # íƒœê·¸ ì¶”ì¶œ
                tags = self._extract_hn_tags(title, content, url)
                
                # ì ìˆ˜ ê³„ì‚°
                final_score = self._calculate_hn_score(title, content, hn_score, comments_count, url)
                
                # HN ì›¹ URL ìƒì„±
                hn_url = f"{self.web_base_url}/item?id={story_id}"
                
                # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                article_data = {
                    'id': f"hackernews_{story_id}_{int(time.time())}",
                    'title': title,
                    'title_ko': title,  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'content': content[:1500] if content else title,  # HNì€ ë³´í†µ ì§§ìŒ
                    'content_ko': content[:1500] if content else title,  # ë²ˆì—­ì€ ë‚˜ì¤‘ì—
                    'summary': '',  # ìš”ì•½ì€ ë‚˜ì¤‘ì—
                    'url': url if url else hn_url,  # ì›ë³¸ ë§í¬ ë˜ëŠ” HN ë§í¬
                    'hn_url': hn_url,  # HN í† ë¡  ë§í¬
                    'source': 'hackernews',
                    'tags': tags,
                    'score': final_score,
                    'hn_score': hn_score,  # ì›ë˜ HN ì ìˆ˜
                    'comments': comments_count,
                    'published': published_time.isoformat(),
                    'author': author,
                    'collected_at': datetime.now(timezone.utc).isoformat()
                }
                
                if final_score > 0:  # ìµœì†Œ ì ìˆ˜ í†µê³¼
                    articles.append(article_data)
                
                # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±ì‹œ ì¢…ë£Œ
                if len(articles) >= limit:
                    break
                
                # API ì œí•œ ë°©ì§€
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"Hacker News ìŠ¤í† ë¦¬ {story_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"Hacker Newsì—ì„œ {len(articles)}ê°œ ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return articles


if __name__ == '__main__':
    # ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸
    config = Config()
    collector = HackerNewsCollector(config)
    
    print("ğŸ§ª Hacker News ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    articles = collector.collect_from_hackernews(limit=5)
    print(f"\nìˆ˜ì§‘ëœ ê¸€ ìˆ˜: {len(articles)}")
    
    for i, article in enumerate(articles, 1):
        print(f"\n{i}. {article['title'][:80]}...")
        print(f"   ìµœì¢… ì ìˆ˜: {article['score']:.1f}")
        print(f"   HN ì ìˆ˜: {article['hn_score']} | ëŒ“ê¸€: {article['comments']}ê°œ")
        print(f"   íƒœê·¸: {', '.join(article['tags'])}")
        print(f"   ì‘ì„±ì: {article['author']}")
        print(f"   URL: {article['url']}")
        print(f"   HN í† ë¡ : {article['hn_url']}")
    
    print(f"\nâœ… Hacker News ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
