#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DS News Aggregator - Main Pipeline
ìˆ˜ì§‘ â†’ í•„í„°ë§ â†’ ë²ˆì—­ â†’ ìš”ì•½ â†’ JSON ì €ì¥ ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
"""

import os
import json
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import time

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from config import Config
from collectors.news_media_collector import NewsMediaCollector
from collectors.practical_blog_collector import PracticalBlogCollector
from collectors.company_blog_collector import CompanyBlogCollector
from collectors.content_filter import ContentFilter
from processors.translator import Translator
from processors.summarizer import Summarizer

logger = logging.getLogger(__name__)

class DSNewsPipeline:
    """DS News Aggregator ì „ì²´ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config = None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config()
        
        # PRD v2.0 - ê° ë‹¨ê³„ë³„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.news_media_collector = NewsMediaCollector(self.config)
        self.practical_blog_collector = PracticalBlogCollector(self.config)
        self.company_blog_collector = CompanyBlogCollector(self.config)
        self.content_filter = ContentFilter(self.config)
        self.translator = Translator(self.config)
        self.summarizer = Summarizer(self.config)
        
        # íŒŒì´í”„ë¼ì¸ í†µê³„
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'stage_times': {},
            'original_articles': 0,
            'filtered_articles': 0,
            'translated_articles': 0,
            'summarized_articles': 0,
            'final_articles': 0,
            'errors': []
        }
    
    def _log_stage_start(self, stage_name: str):
        """ë‹¨ê³„ ì‹œì‘ ë¡œê¹…"""
        logger.info(f"===== {stage_name} ì‹œì‘ =====")
        self.pipeline_stats['stage_times'][stage_name + '_start'] = datetime.now(timezone.utc)
    
    def _log_stage_end(self, stage_name: str, result_count: int):
        """ë‹¨ê³„ ì™„ë£Œ ë¡œê¹…"""
        end_time = datetime.now(timezone.utc)
        start_time = self.pipeline_stats['stage_times'][stage_name + '_start']
        duration = (end_time - start_time).total_seconds()
        
        self.pipeline_stats['stage_times'][stage_name + '_duration'] = duration
        
        logger.info(f"===== {stage_name} ì™„ë£Œ: {result_count}ê°œ ê¸€, {duration:.1f}ì´ˆ =====")
    
    def step1_collect_articles(self) -> List[Dict[str, Any]]:
        """
        1ë‹¨ê³„: PRD v2.0 - ë‰´ìŠ¤ ë¯¸ë””ì–´(50%) + ë¸”ë¡œê·¸(30%) + ê¸°ì—…(20%) ìˆ˜ì§‘
        
        Returns:
            ìˆ˜ì§‘ëœ ëª¨ë“  ê¸€ ëª©ë¡
        """
        self._log_stage_start("ê¸€ ìˆ˜ì§‘")
        
        all_articles = []
        
        try:
            # ë‰´ìŠ¤ ë¯¸ë””ì–´ ìˆ˜ì§‘ (50%)
            logger.info("ë‰´ìŠ¤ ë¯¸ë””ì–´ì—ì„œ AI/ML ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            news_articles = self.news_media_collector.collect(
                max_articles_per_source=self.config.NEWS_MEDIA_MAX_ARTICLES
            )
            all_articles.extend(news_articles)
            logger.info(f"ë‰´ìŠ¤ ë¯¸ë””ì–´ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_articles)}ê°œ")
            
            # ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ (30%)
            logger.info("ì‹¤ìš© ë¸”ë¡œê·¸/ê¿€íŒ í”Œë«í¼ì—ì„œ ìˆ˜ì§‘ ì¤‘...")
            blog_articles = self.practical_blog_collector.collect(
                max_articles_per_source=self.config.PRACTICAL_BLOG_MAX_ARTICLES
            )
            all_articles.extend(blog_articles)
            logger.info(f"ì‹¤ìš© ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_articles)}ê°œ")
            
            # ê¸°ì—… ë¸”ë¡œê·¸ ìˆ˜ì§‘ (20%)
            logger.info("ê¸°ì—… ê¸°ìˆ  ë¸”ë¡œê·¸ì—ì„œ ìˆ˜ì§‘ ì¤‘...")
            company_articles = self.company_blog_collector.collect(
                max_articles_per_source=self.config.COMPANY_BLOG_MAX_ARTICLES
            )
            all_articles.extend(company_articles)
            logger.info(f"ê¸°ì—… ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(company_articles)}ê°œ")
            
            # ìˆ˜ì§‘ ë¹„ìœ¨ ë¶„ì„
            news_ratio = len(news_articles) / len(all_articles) * 100 if all_articles else 0
            blog_ratio = len(blog_articles) / len(all_articles) * 100 if all_articles else 0
            company_ratio = len(company_articles) / len(all_articles) * 100 if all_articles else 0
            
            logger.info(f"ìˆ˜ì§‘ ë¹„ìœ¨ - ë‰´ìŠ¤: {news_ratio:.1f}%, ë¸”ë¡œê·¸: {blog_ratio:.1f}%, ê¸°ì—…: {company_ratio:.1f}%")
            
        except Exception as e:
            error_msg = f"ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
        
        self.pipeline_stats['original_articles'] = len(all_articles)
        self._log_stage_end("ê¸€ ìˆ˜ì§‘", len(all_articles))
        
        return all_articles
    
    def step2_filter_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        2ë‹¨ê³„: PRD v2.0 ì½˜í…ì¸  í•„í„°ë§ (ë‰´ìŠ¤ 50%, ë¸”ë¡œê·¸ 30%, ê¸°ì—… 20% ë¹„ìœ¨ ìœ ì§€)
        
        Args:
            articles: í•„í„°ë§í•  ê¸€ ëª©ë¡
            
        Returns:
            í•„í„°ë§ëœ ê¸€ ëª©ë¡
        """
        self._log_stage_start("ì½˜í…ì¸  í•„í„°ë§")
        
        try:
            # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
            news_articles = [a for a in articles if a.get('source_id', '').startswith(('techcrunch', 'venturebeat', 'mit_tech', 'ai_times', 'zdnet', 'tech42'))]
            blog_articles = [a for a in articles if a.get('source_id', '').startswith(('towards_data', 'analytics_vidhya', 'kdnuggets', 'neptune'))]
            company_articles = [a for a in articles if a.get('source_id', '').startswith(('google_ai', 'openai', 'naver_d2', 'kakao_tech'))]
            
            logger.info(f"ì†ŒìŠ¤ë³„ ë¶„ë¥˜: ë‰´ìŠ¤ {len(news_articles)}ê°œ, ë¸”ë¡œê·¸ {len(blog_articles)}ê°œ, ê¸°ì—… {len(company_articles)}ê°œ")
            
            # PRD v2.0 ë¹„ìœ¨ì— ë”°ë¥¸ ì„ ë³„ (5-10ê°œ ì´ ëª©í‘œ)
            target_total = min(10, len(articles))  # ìµœëŒ€ 10ê°œ
            
            # ë‰´ìŠ¤ 3-5ê°œ (50%)
            news_target = max(3, min(5, int(target_total * self.config.NEWS_MEDIA_RATIO)))
            news_filtered = self.content_filter.get_top_articles(news_articles)[:news_target]
            
            # ë¸”ë¡œê·¸ 2-3ê°œ (30%)
            blog_target = max(2, min(3, int(target_total * self.config.PRACTICAL_BLOG_RATIO)))
            blog_filtered = self.content_filter.get_top_articles(blog_articles)[:blog_target]
            
            # ê¸°ì—… 1-2ê°œ (20%)
            company_target = max(1, min(2, int(target_total * self.config.COMPANY_BLOG_RATIO)))
            company_filtered = self.content_filter.get_top_articles(company_articles)[:company_target]
            
            # ìµœì¢… ê²°í•©
            filtered_articles = news_filtered + blog_filtered + company_filtered
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            filtered_articles.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            self.pipeline_stats['filtered_articles'] = len(filtered_articles)
            
            logger.info(f"ìµœì¢… ì„ ë³„: ë‰´ìŠ¤ {len(news_filtered)}ê°œ, ë¸”ë¡œê·¸ {len(blog_filtered)}ê°œ, ê¸°ì—… {len(company_filtered)}ê°œ = ì´ {len(filtered_articles)}ê°œ")
            
            # í•„í„°ë§ ë¶„ì„ ê²°ê³¼
            analysis = self.content_filter.analyze_filtering_results(articles, filtered_articles)
            logger.info(f"í•„í„°ë§ ë¶„ì„: {analysis}")
            
        except Exception as e:
            error_msg = f"ì½˜í…ì¸  í•„í„°ë§ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í•„í„°ë§ ì ìš©
            filtered_articles = self.content_filter.get_top_articles(articles)
        
        self._log_stage_end("ì½˜í…ì¸  í•„í„°ë§", len(filtered_articles))
        return filtered_articles
    
    def step3_translate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        3ë‹¨ê³„: ê¸€ ë²ˆì—­ (ì˜ë¬¸ë§Œ í•œêµ­ì–´ë¡œ ë²ˆì—­, 2000ì ì´ìƒì€ 1000ìë§Œ)
        
        Args:
            articles: ë²ˆì—­í•  ê¸€ ëª©ë¡
            
        Returns:
            ë²ˆì—­ëœ ê¸€ ëª©ë¡
        """
        self._log_stage_start("ê¸€ ë²ˆì—­")
        
        try:
            # ë²ˆì—­ì´ í•„ìš”í•œ ê¸€ë§Œ í•„í„°ë§
            articles_to_translate = [
                article for article in articles 
                if article.get('needs_translation', False)
            ]
            
            if articles_to_translate:
                logger.info(f"{len(articles_to_translate)}ê°œ ê¸€ ë²ˆì—­ ì‹œì‘...")
                translated_articles = self.translator.translate_articles_batch(articles_to_translate)
                
                # ë²ˆì—­ ê²°ê³¼ë¥¼ ì›ë˜ ë¦¬ìŠ¤íŠ¸ì— ë°˜ì˜
                translated_dict = {a['id']: a for a in translated_articles}
                
                result_articles = []
                for article in articles:
                    if article['id'] in translated_dict:
                        result_articles.append(translated_dict[article['id']])
                    else:
                        result_articles.append(article)
                
                self.pipeline_stats['translated_articles'] = len(articles_to_translate)
            else:
                logger.info("ë²ˆì—­ì´ í•„ìš”í•œ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                result_articles = articles
                self.pipeline_stats['translated_articles'] = 0
            
            # ë²ˆì—­ í†µê³„
            translation_stats = self.translator.get_translation_stats()
            logger.info(f"ë²ˆì—­ í†µê³„: {translation_stats}")
            
        except Exception as e:
            error_msg = f"ê¸€ ë²ˆì—­ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
            result_articles = articles  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
        self._log_stage_end("ê¸€ ë²ˆì—­", len(result_articles))
        return result_articles
    
    def step4_summarize_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        4ë‹¨ê³„: ê¸€ ìš”ì•½ (Gemini Proë¡œ ì •í™•íˆ 3ë¬¸ì¥ ìš”ì•½)
        
        Args:
            articles: ìš”ì•½í•  ê¸€ ëª©ë¡
            
        Returns:
            ìš”ì•½ëœ ê¸€ ëª©ë¡
        """
        self._log_stage_start("ê¸€ ìš”ì•½")
        
        try:
            logger.info(f"{len(articles)}ê°œ ê¸€ ìš”ì•½ ì‹œì‘...")
            
            summarized_articles = self.summarizer.summarize_articles_batch(articles)
            self.pipeline_stats['summarized_articles'] = len(summarized_articles)
            
            # ìš”ì•½ í†µê³„
            summary_stats = self.summarizer.get_summarization_stats()
            logger.info(f"ìš”ì•½ í†µê³„: {summary_stats}")
            
            # í‚¬ìŠ¤ìœ„ì¹˜ ìƒíƒœ í™•ì¸
            if summary_stats.get('killswitch_active', False):
                logger.warning("âš ï¸  Gemini API í‚¬ìŠ¤ìœ„ì¹˜ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
        except Exception as e:
            error_msg = f"ê¸€ ìš”ì•½ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
            summarized_articles = articles  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
        self._log_stage_end("ê¸€ ìš”ì•½", len(summarized_articles))
        return summarized_articles
    
    def step5_save_articles(self, articles: List[Dict[str, Any]]) -> bool:
        """
        5ë‹¨ê³„: JSON ì €ì¥ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ í˜•ì‹)
        
        Args:
            articles: ì €ì¥í•  ê¸€ ëª©ë¡
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        self._log_stage_start("JSON ì €ì¥")
        
        try:
            # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(self.config.ARTICLES_FILE), exist_ok=True)
            
            # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” JSON í˜•ì‹
            today = datetime.now(timezone.utc).date().isoformat()
            
            output_data = {
                "date": today,
                "articles": articles
            }
            
            # ë©”ì¸ íŒŒì¼ ì €ì¥
            with open(self.config.ARTICLES_FILE, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            # íˆìŠ¤í† ë¦¬ íŒŒì¼ ì €ì¥
            history_file = os.path.join(self.config.DATA_DIR, f'articles_{today}.json')
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            self.pipeline_stats['final_articles'] = len(articles)
            
            logger.info(f"JSON ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - ë©”ì¸ íŒŒì¼: {self.config.ARTICLES_FILE}")
            logger.info(f"  - íˆìŠ¤í† ë¦¬: {history_file}")
            logger.info(f"  - ì´ ê¸€ ìˆ˜: {len(articles)}ê°œ")
            
            self._log_stage_end("JSON ì €ì¥", len(articles))
            return True
            
        except Exception as e:
            error_msg = f"JSON ì €ì¥ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
            self._log_stage_end("JSON ì €ì¥", 0)
            return False
    
    def run_full_pipeline(self) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ìˆ˜ì§‘ â†’ í•„í„°ë§ â†’ ë²ˆì—­ â†’ ìš”ì•½ â†’ JSON ì €ì¥
        
        Returns:
            íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ í†µê³„
        """
        logger.info("ğŸš€ DS News Aggregator íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.pipeline_stats['start_time'] = datetime.now(timezone.utc)
        
        try:
            # 1ë‹¨ê³„: ê¸€ ìˆ˜ì§‘
            articles = self.step1_collect_articles()
            
            if not articles:
                logger.warning("ìˆ˜ì§‘ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return self.get_pipeline_stats()
            
            # 2ë‹¨ê³„: ì½˜í…ì¸  í•„í„°ë§
            articles = self.step2_filter_articles(articles)
            
            if not articles:
                logger.warning("í•„í„°ë§ í›„ ë‚¨ì€ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return self.get_pipeline_stats()
            
            # 3ë‹¨ê³„: ê¸€ ë²ˆì—­
            articles = self.step3_translate_articles(articles)
            
            # 4ë‹¨ê³„: ê¸€ ìš”ì•½
            articles = self.step4_summarize_articles(articles)
            
            # 5ë‹¨ê³„: JSON ì €ì¥
            success = self.step5_save_articles(articles)
            
            if success:
                logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            else:
                logger.error("ğŸ’¥ íŒŒì´í”„ë¼ì¸ ì €ì¥ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨")
            
        except Exception as e:
            error_msg = f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            self.pipeline_stats['errors'].append(error_msg)
        
        finally:
            self.pipeline_stats['end_time'] = datetime.now(timezone.utc)
        
        return self.get_pipeline_stats()
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í†µê³„ ë°˜í™˜
        
        Returns:
            ì‹¤í–‰ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        stats = self.pipeline_stats.copy()
        
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            stats['total_duration_seconds'] = duration.total_seconds()
            stats['total_duration_str'] = str(duration).split('.')[0]
        
        return stats
    
    def run_pipeline_step_by_step(self, save_intermediates: bool = True) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰ (ë””ë²„ê¹…/ê°œë°œìš©)
        
        Args:
            save_intermediates: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ í†µê³„
        """
        logger.info("ğŸ”§ DS News Aggregator íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹¤í–‰")
        
        # ê° ë‹¨ê³„ë³„ ê²°ê³¼ ì €ì¥
        step_results = {}
        
        # 1ë‹¨ê³„
        articles = self.step1_collect_articles()
        step_results['step1_collect'] = articles
        
        if save_intermediates:
            with open('data/step1_collected.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
        
        if not articles:
            return self.get_pipeline_stats()
        
        # 2ë‹¨ê³„
        articles = self.step2_filter_articles(articles)
        step_results['step2_filter'] = articles
        
        if save_intermediates:
            with open('data/step2_filtered.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
        
        if not articles:
            return self.get_pipeline_stats()
        
        # 3ë‹¨ê³„
        articles = self.step3_translate_articles(articles)
        step_results['step3_translate'] = articles
        
        if save_intermediates:
            with open('data/step3_translated.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
        
        # 4ë‹¨ê³„
        articles = self.step4_summarize_articles(articles)
        step_results['step4_summarize'] = articles
        
        if save_intermediates:
            with open('data/step4_summarized.json', 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
        
        # 5ë‹¨ê³„
        self.step5_save_articles(articles)
        step_results['step5_final'] = articles
        
        logger.info("ğŸ“Š ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return self.get_pipeline_stats()


# í¸ì˜ í•¨ìˆ˜ë“¤
def run_ds_news_pipeline(config: Config = None) -> Dict[str, Any]:
    """
    DS News Aggregator íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • ê°ì²´
        
    Returns:
        ì‹¤í–‰ ê²°ê³¼ í†µê³„
    """
    pipeline = DSNewsPipeline(config)
    return pipeline.run_full_pipeline()


def run_pipeline_with_custom_steps(collect: bool = True, 
                                  filter_content: bool = True,
                                  translate: bool = True, 
                                  summarize: bool = True,
                                  save: bool = True,
                                  config: Config = None) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì •ì˜ ë‹¨ê³„ë¡œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        collect: ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ ì—¬ë¶€
        filter_content: í•„í„°ë§ ë‹¨ê³„ ì‹¤í–‰ ì—¬ë¶€
        translate: ë²ˆì—­ ë‹¨ê³„ ì‹¤í–‰ ì—¬ë¶€
        summarize: ìš”ì•½ ë‹¨ê³„ ì‹¤í–‰ ì—¬ë¶€
        save: ì €ì¥ ë‹¨ê³„ ì‹¤í–‰ ì—¬ë¶€
        config: ì„¤ì • ê°ì²´
        
    Returns:
        ì‹¤í–‰ ê²°ê³¼ í†µê³„
    """
    pipeline = DSNewsPipeline(config)
    pipeline.pipeline_stats['start_time'] = datetime.now(timezone.utc)
    
    articles = []
    
    try:
        if collect:
            articles = pipeline.step1_collect_articles()
            if not articles:
                return pipeline.get_pipeline_stats()
        
        if filter_content and articles:
            articles = pipeline.step2_filter_articles(articles)
            if not articles:
                return pipeline.get_pipeline_stats()
        
        if translate and articles:
            articles = pipeline.step3_translate_articles(articles)
        
        if summarize and articles:
            articles = pipeline.step4_summarize_articles(articles)
        
        if save and articles:
            pipeline.step5_save_articles(articles)
            
    except Exception as e:
        logger.error(f"ì‚¬ìš©ì ì •ì˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        pipeline.pipeline_stats['errors'].append(str(e))
    
    finally:
        pipeline.pipeline_stats['end_time'] = datetime.now(timezone.utc)
    
    return pipeline.get_pipeline_stats()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ëª…ë ¹ì¤„ ì¸ìì— ë”°ë¥¸ ì‹¤í–‰ ëª¨ë“œ
    if len(sys.argv) > 1 and sys.argv[1] == 'debug':
        # ë””ë²„ê·¸ ëª¨ë“œ: ë‹¨ê³„ë³„ ì‹¤í–‰
        pipeline = DSNewsPipeline()
        stats = pipeline.run_pipeline_step_by_step()
    else:
        # ì¼ë°˜ ëª¨ë“œ: ì „ì²´ ì‹¤í–‰
        stats = run_ds_news_pipeline()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼")
    print("="*50)
    print(f"ì „ì²´ ì†Œìš” ì‹œê°„: {stats.get('total_duration_str', 'N/A')}")
    print(f"ìˆ˜ì§‘ëœ ê¸€: {stats['original_articles']}ê°œ")
    print(f"í•„í„°ë§ëœ ê¸€: {stats['filtered_articles']}ê°œ")
    print(f"ë²ˆì—­ëœ ê¸€: {stats['translated_articles']}ê°œ")
    print(f"ìš”ì•½ëœ ê¸€: {stats['summarized_articles']}ê°œ")
    print(f"ìµœì¢… ì €ì¥ëœ ê¸€: {stats['final_articles']}ê°œ")
    
    if stats['errors']:
        print(f"ì˜¤ë¥˜ {len(stats['errors'])}ê°œ:")
        for error in stats['errors']:
            print(f"  - {error}")
    
    print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
